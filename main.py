# ThesaurusV2

import pandas as pd
from datetime import datetime, timedelta, time
import os
import requests
from requests.exceptions import RequestException
import matplotlib.pyplot as plt
from dotenv import load_dotenv
import logging
import matplotlib.dates as mdates
import re

import sys


import holidays
from datetime import date

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
log_dir = os.path.join(os.getcwd(), "log")
os.makedirs(log_dir, exist_ok=True)

# –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ –ª–æ–≥-—Ñ–∞–π–ª—É
log_file = os.path.join(log_dir, "ruonia_log.txt")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(
    filename=log_file,
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    encoding="utf-8"
)

logger = logging.getLogger(__name__)


def is_russian_workday(check_date=None):
    if check_date is None:
        check_date = date.today()

    ru_holidays = holidays.Russia()




    return check_date not in ru_holidays # –≤—ã—Ö–æ–¥–Ω—ã–µ —Å–± –≤—Å –∏    
    # return check_date.weekday() < 5 and check_date not in ru_holidays # –≤—ã—Ö–æ–¥–Ω—ã–µ —Å–± –≤—Å –∏

#### 
# ======================= WORDSTAT: –∫—Ä–∏–∑–∏—Å–Ω—ã–µ –∫–ª—é—á–∏ -> –≥—Ä–∞—Ñ–∏–∫–∏ -> —Ä–∞—Å—Å—ã–ª–∫–∞ =======================
import os
import re
import json
import time
import requests
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta, date
from dateutil.relativedelta import relativedelta
from dotenv import load_dotenv

load_dotenv()

# –∏–∑ .env:
#  - WORDSTAT_OAUTH  ‚Äî OAuth-—Ç–æ–∫–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π —Ç—ã –ø–æ–ª—É—á–∏–ª –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ ...
#  - WORDSTAT_CLIENT_ID ‚Äî ClientId –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (—Å —Ç–æ–π –∂–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
WORDSTAT_OAUTH = os.getenv("WORDSTAT_OAUTH")
WORDSTAT_CLIENT_ID = os.getenv("WORDSTAT_CLIENT_ID")

# –±–∞–∑–æ–≤—ã–π URL API –∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫–∏
WORDSTAT_API = "https://api.wordstat.yandex.net"
WORDSTAT_REGION_RUSSIA = 225  # –†–æ—Å—Å–∏—è
WORDSTAT_SAVE_DIR = os.path.join("src", "wordstat")
WORDSTAT_RAW_DIR = os.path.join(WORDSTAT_SAVE_DIR, "raw")  # —Å—é–¥–∞ –±—É–¥–µ–º —Å–∫–ª–∞–¥—ã–≤–∞—Ç—å –≤—Å–µ –æ—Ç–≤–µ—Ç—ã API
os.makedirs(WORDSTAT_SAVE_DIR, exist_ok=True)
os.makedirs(WORDSTAT_RAW_DIR, exist_ok=True)

# –∫–ª–∏–µ–Ω—Ç Perplexity –∏–∑ —Ç–≤–æ–µ–≥–æ ai.py
from functions.ai import client as ai_client


def _last_sunday_on_or_before(d: date) -> date:
    # Monday=0..Sunday=6 -> –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ –ø—Ä–æ—à–µ–¥—à–µ–≥–æ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å—è
    return d - timedelta(days=(d.weekday() + 1) % 7)


def _first_monday_on_or_after(d: date) -> date:
    return d + timedelta(days=(7 - d.weekday()) % 7)


def _prepare_week_bounds():
    """
    fromDate: –ø–µ—Ä–≤—ã–π –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫ >= 2018-01-01
    toDate: –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ –Ω–µ –ø–æ–∑–¥–Ω–µ–µ —Å–µ–≥–æ–¥–Ω—è-2
    """
    today = datetime.today().date()
    to_date = today - timedelta(days=2)
    to_date = _last_sunday_on_or_before(to_date)

    from_date = date(2018, 1, 1)
    from_date = _first_monday_on_or_after(from_date)
    return from_date, to_date


def _wordstat_post(path: str, payload: dict, retries: int = 4, backoff: float = 1.5):
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π POST –∫ Wordstat API.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä—è–º–æ–π OAuth-—Ç–æ–∫–µ–Ω (Bearer), –∫–∞–∫ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫–µ:
    https://yandex.ru/support2/wordstat/ru/content/api-wordstat
    """
    if not WORDSTAT_OAUTH or not WORDSTAT_CLIENT_ID:
        raise RuntimeError("WORDSTAT_OAUTH –∏–ª–∏ WORDSTAT_CLIENT_ID –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ .env")

    url = f"{WORDSTAT_API}{path}"

    headers = {
        "Authorization": f"Bearer {WORDSTAT_OAUTH}",
        "Content-Type": "application/json; charset=utf-8",
        "X-Client-Id": WORDSTAT_CLIENT_ID,
    }

    for attempt in range(1, retries + 1):
        r = requests.post(url, headers=headers, data=json.dumps(payload), timeout=30)

        if r.status_code == 200:
            return r.json()

        # 429 / 503 ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –±—ç–∫–æ—Ñ—Ñ
        if r.status_code in (429, 503):
            wait = backoff ** attempt
            logger.warning(f"Wordstat {path} –≤–µ—Ä–Ω—É–ª {r.status_code}. –†–µ—Ç—Ä–∞–π —á–µ—Ä–µ–∑ {wait:.1f}s...")
            time.sleep(wait)
            continue

        # –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ—à–∏–±–∫–∏ ‚Äî –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º —Å —Ç–µ–ª–æ–º
        try:
            err = r.json()
        except Exception:
            err = r.text
        raise RuntimeError(f"Wordstat error {r.status_code}: {err}")

    raise RuntimeError(f"Wordstat {path} –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –ø–æ—Å–ª–µ {retries} –ø–æ–ø—ã—Ç–æ–∫")


def get_crisis_keywords_via_perplexity() -> list[str]:
    """
    –ë–µ—Ä—ë–º 8 —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –ø—Ä–æ –∫—Ä–∏–∑–∏—Å—ã/–ø—Ä–æ–±–ª–µ–º—ã/—Å—Ç—Ä–µ—Å—Å—ã –Ω–∞—Å–µ–ª–µ–Ω–∏—è
    —á–µ—Ä–µ–∑ —Ç–æ–≥–æ –∂–µ –∫–ª–∏–µ–Ω—Ç–∞, —á—Ç–æ –∏ run_brief() (ai.py).

    –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ ‚Äî JSON-–º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫ (—Ä–æ–≤–Ω–æ 8).
    """
    sys_prompt = (
        "–°—Ñ–æ—Ä–º–∏—Ä—É–π 8 —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∫—Ä–∞—Ç–∫–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑, –ø–æ –∫–æ—Ç–æ—Ä—ã–º –º–æ–∂–Ω–æ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ —Å—Ç—Ä–∞—Ö–∏ –ª—é–¥–µ–π –≤–æ –≤—Ä–µ–º—è —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏—Ö/—Å–æ—Ü–∏–∞–ª—å–Ω—ã—Ö –∫—Ä–∏–∑–∏—Å–æ–≤, "
        "–¥–æ–ª–∂–Ω–æ –æ—Ç—Ä–∞–∂–∞—Ç—å: —Ñ–∏–Ω–∞–Ω—Å—ã, –∑–∞–Ω—è—Ç–æ—Å—Ç—å, –±–∞–Ω–∫–∏, —Ü–µ–Ω—ã, –≤–∞–ª—é—Ç—É, –¥–æ–ª–≥–∏, –æ—Ç–∫–ª—é—á–µ–Ω–∏—è, –∑–¥–æ—Ä–æ–≤—å–µ/–∞–ø—Ç–µ–∫–∏. "
        "–¢–æ–ª—å–∫–æ JSON-–º–∞—Å—Å–∏–≤ –∏–∑ —Ä–æ–≤–Ω–æ 8 —Å—Ç—Ä–æ–∫ –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π, –ø—Ä–∏–º–µ—Ä: [\"–æ–±–≤–∞–ª —Ä—É–±–ª—è\", \"—Ä–æ—Å—Ç —Ü–µ–Ω\" ...]"
    )
    resp = ai_client.chat.completions.create(
        model="sonar-pro",
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": "–î–∞–π –º–∞—Å—Å–∏–≤ –∏–∑ 8 —Ñ—Ä–∞–∑. –¢–æ–ª—å–∫–æ JSON, –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ/–ø–æ—Å–ª–µ."}
        ],
        temperature=0.2,
        top_p=0.9,
        max_tokens=400,
        stream=False,
    )
    raw = resp.choices[0].message.content.strip()
    try:
        arr = json.loads(raw)
        if not isinstance(arr, list) or len(arr) != 8:
            raise ValueError("–û–∂–∏–¥–∞–ª—Å—è –º–∞—Å—Å–∏–≤ –∏–∑ 8 —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        return [str(x).strip() for x in arr]
    except Exception as e:
        logger.warning(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –æ—Ç Perplexity –Ω–µ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–∏—Å—å: {e}. –ò—Å–ø–æ–ª—å–∑—É—é –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –Ω–∞–±–æ—Ä.")
        return [
            "–æ–±–≤–∞–ª —Ä—É–±–ª—è", "—Ä–æ—Å—Ç —Ü–µ–Ω", "–¥–µ—Ñ–æ–ª—Ç", "–±–µ–∑—Ä–∞–±–æ—Ç–∏—Ü–∞",
            "–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞–Ω–∏–∫—É–ª—ã", "–∑–∞–∫—Ä—ã—Ç–∏–µ –±–∞–Ω–∫–æ–≤", "–¥–µ—Ñ–∏—Ü–∏—Ç –ª–µ–∫–∞—Ä—Å—Ç–≤", "–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–∞"
        ]


def fetch_wordstat_dynamics(phrase: str, regions=None, devices=None) -> pd.DataFrame:
    """
    –î–æ—Å—Ç–∞—ë—Ç weekly-–¥–∏–Ω–∞–º–∏–∫—É counts/share –ø–æ —Ñ—Ä–∞–∑–µ —Å 2018-01-01 –¥–æ —Å–µ–≥–æ–¥–Ω—è-2 (–ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ /v1/dynamics.
    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç *—Å—ã—Ä–æ–π* –æ—Ç–≤–µ—Ç API –≤ JSON-—Ñ–∞–π–ª –≤ src/wordstat/raw.
    """
    if regions is None:
        regions = [WORDSTAT_REGION_RUSSIA]
    if devices is None:
        devices = ["all"]

    from_date, to_date = _prepare_week_bounds()
    payload = {
        "phrase": phrase,  # –≤ —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –¥–æ–ø—É—Å—Ç–∏–º —Ç–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä '+', –Ω–æ –ø—Ä–æ—Å—Ç–∞—è —Ñ—Ä–∞–∑–∞ —Ç–æ–∂–µ –æ–∫
        "period": "weekly",
        "fromDate": from_date.strftime("%Y-%m-%d"),
        "toDate": to_date.strftime("%Y-%m-%d"),
        "regions": regions,
        "devices": devices,
    }
    data = _wordstat_post("/v1/dynamics", payload)

    # === –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –≤ JSON ===
    try:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_phrase = re.sub(r"[^a-zA-Z0-9_.-]+", "_", phrase.strip())
        raw_fname = f"wordstat_dynamics_{safe_phrase}_{ts}.json"
        raw_fpath = os.path.join(WORDSTAT_RAW_DIR, raw_fname)
        with open(raw_fpath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"üíæ –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç Wordstat –ø–æ '{phrase}' —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {raw_fpath}")
    except Exception as e:
        logger.exception(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç Wordstat –ø–æ '{phrase}': {e}")

    # === –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame ===
    dyn = data.get("dynamics", [])
    df = pd.DataFrame(dyn)
    if df.empty:
        df = pd.DataFrame(columns=["date", "count", "share"])
    else:
        df["date"] = pd.to_datetime(df["date"])
        df = df.sort_values("date").reset_index(drop=True)
    df["phrase"] = phrase
    return df


def build_and_save_charts(df_all: pd.DataFrame, out_dir: str = WORDSTAT_SAVE_DIR) -> list[str]:
    """
    –ü–æ –∫–∞–∂–¥–æ–π —Ñ—Ä–∞–∑–µ —Ä–∏—Å—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π PNG: count –≤–æ –≤—Ä–µ–º–µ–Ω–∏ (share –Ω–µ —Ä–∏—Å—É–µ–º –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã).
    """
    saved = []
    for phrase, df in df_all.groupby("phrase"):
        if df.empty:
            continue
        plt.figure(figsize=(12, 5))
        plt.plot(df["date"], df["count"], linewidth=2)
        plt.title(f"Wordstat weekly: {phrase}")
        plt.xlabel("–î–∞—Ç–∞ (–Ω–µ–¥–µ–ª–∏)")
        plt.ylabel("–ß–∏—Å–ª–æ –∑–∞–ø—Ä–æ—Å–æ–≤")
        plt.grid(True)
        plt.tight_layout()

        safe_phrase = re.sub(r"[^a-zA-Z0-9_.-]+", "_", phrase.strip())
        fname = f"wordstat_{safe_phrase}.png"
        fpath = os.path.join(out_dir, fname)
        base, ext = os.path.splitext(fpath)
        k = 1
        while os.path.exists(fpath):
            k += 1
            fpath = f"{base}_{k}{ext}"

        plt.savefig(fpath)
        plt.close()
        saved.append(fpath)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—ë–Ω –≥—Ä–∞—Ñ–∏–∫: {fpath}")
    return saved


def send_wordstat_digest(tg_client, recipients):
    """
    –ì–ª–∞–≤–Ω—ã–π —Ä–∞–Ω–Ω–µ—Ä:
      1) –±–µ—Ä—ë–º 8 –∫–ª—é—á–µ–π —É Perplexity,
      2) —Ç—è–Ω–µ–º –¥–∏–Ω–∞–º–∏–∫—É –∏–∑ Wordstat (–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ JSON-–æ—Ç–≤–µ—Ç—ã),
      3) —Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏,
      4) —Ä–∞—Å—Å—ã–ª–∞–µ–º –≤ TG.

    –ü—Ä–∏ –ª—é–±–æ–π –æ—à–∏–±–∫–µ Wordstat –ø—Ä–æ—Å—Ç–æ —à–ª—ë–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, –Ω–µ –≤–∞–ª–∏–º –≤—Å—é –ø—Ä–æ–≥—Ä–∞–º–º—É.
    """
    # 1) –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
    try:
        keywords = get_crisis_keywords_via_perplexity()
    except Exception as e:
        logger.exception(f"Perplexity –Ω–µ –≤–µ—Ä–Ω—É–ª –∫–ª—é—á–∏: {e}")
        keywords = [
            "–æ–±–≤–∞–ª —Ä—É–±–ª—è", "—Ä–æ—Å—Ç —Ü–µ–Ω", "–¥–µ—Ñ–æ–ª—Ç", "–±–µ–∑—Ä–∞–±–æ—Ç–∏—Ü–∞",
            "–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞–Ω–∏–∫—É–ª—ã", "–∑–∞–∫—Ä—ã—Ç–∏–µ –±–∞–Ω–∫–æ–≤", "–¥–µ—Ñ–∏—Ü–∏—Ç –ª–µ–∫–∞—Ä—Å—Ç–≤", "–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–∞"
        ]

    logger.info(f"Wordstat: –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã: {keywords}")

    # 2) –¥–∏–Ω–∞–º–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É
    frames = []
    errors = []
    for kw in keywords:
        try:
            df_kw = fetch_wordstat_dynamics(kw)
            if not df_kw.empty:
                frames.append(df_kw)
                logger.info(f"Wordstat: –ø–æ —Ñ—Ä–∞–∑–µ '{kw}' –ø–æ–ª—É—á–µ–Ω–æ {len(df_kw)} —Ç–æ—á–µ–∫")
            else:
                logger.warning(f"Wordstat: –ø–æ —Ñ—Ä–∞–∑–µ '{kw}' –ø—Ä–∏—à—ë–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç (dynamics=[])")
            time.sleep(0.3)  # —á—É—Ç–∫–∞ –ø–æ–¥–¥—Ä–æ—Å–∏–º, —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∞–Ω–∏–ª–∏ –ø–æ RPS
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ Wordstat –ø–æ '{kw}': {e}")
            errors.append((kw, str(e)))

    # –ï—Å–ª–∏ –ù–ò –ø–æ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî —à–ª—ë–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –∏ –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏
    if not frames:
        msg = (
            "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –í–æ—Ä–¥–∞ –Ω–∏ –ø–æ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–µ.\n"
            "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
            "‚Ä¢ –Ω–µ–≤–µ—Ä–Ω—ã–π WORDSTAT_OAUTH –∏–ª–∏ WORDSTAT_CLIENT_ID;\n"
            "‚Ä¢ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é –Ω–µ –≤—ã–¥–∞–Ω –¥–æ—Å—Ç—É–ø –∫ API –í–æ—Ä–¥–∞;\n"
            "‚Ä¢ –∏—Å—á–µ—Ä–ø–∞–Ω–∞ –∫–≤–æ—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç RPS;\n"
            "‚Ä¢ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞.\n\n"
            "–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ —Å–º–æ—Ç—Ä–∏ –≤ –ª–æ–≥–∞—Ö (–∏—â–∏ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º 'Wordstat')."
        )
        for chat_id in recipients:
            try:
                tg_client.send_message(chat_id, msg)
            except Exception as e:
                logger.exception(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ {chat_id}: {e}")
        return

    # 3) —Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏
    all_df = pd.concat(frames, ignore_index=True)
    files = build_and_save_charts(all_df)

    if not files:
        msg = (
            "‚ö†Ô∏è Wordstat –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ "
            "(–≤–æ–∑–º–æ–∂–Ω–æ, –≤—Å–µ DataFrame –æ–∫–∞–∑–∞–ª–∏—Å—å –ø—É—Å—Ç—ã–º–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)."
        )
        for chat_id in recipients:
            try:
                tg_client.send_message(chat_id, msg)
            except Exception as e:
                logger.exception(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ {chat_id}: {e}")
        return

    # 4) —Ä–∞—Å—Å—ã–ª–∫–∞ –≤ Telegram
    header = (
        "üìä –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–í–æ—Ä–¥)\n"
        "–ü–µ—Ä–∏–æ–¥: —Å 2018-01-01 –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–µ–¥–µ–ª–∏.\n"
        "–ò—Å—Ç–æ—á–Ω–∏–∫: API /v1/dynamics."
    )
    for chat_id in recipients:
        try:
            tg_client.send_message(chat_id, header)
            for f in files:
                tg_client.send_photo(chat_id, photo=f, caption=os.path.basename(f))
            logger.info(f"‚úÖ Wordstat-–¥–∞–π–¥–∂–µ—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ {chat_id}")
        except Exception as e:
            logger.exception(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ {chat_id}: {e}")

# ======================= /WORDSTAT =======================






# –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–∞ –∫–∞–∫–æ–π –ø—Ä–æ–º–µ–∂—É—Ç–µ–∫ –Ω—É–∂–µ–Ω –∑–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ —Å–µ–≥–æ–¥–Ω—è, 
def check_if_need_new_rec(FILENAME="ruonia_data.xlsx"):
    try:
        today = datetime.today().date()
        end_date = today.strftime("%d.%m.%Y")
        start_date = "11.01.2010"
        from_dt = datetime.strptime(start_date, "%d.%m.%Y").strftime("%m/%d/%Y")
        to_dt = today.strftime("%m/%d/%Y")

        if os.path.exists(FILENAME):
            try:
                local_df = pd.read_excel(FILENAME)
                local_df["–î–∞—Ç–∞"] = pd.to_datetime(local_df["–î–∞—Ç–∞"], dayfirst=True)
                last_date = local_df["–î–∞—Ç–∞"].max().date()
                from_date = local_df["–î–∞—Ç–∞"].min().date()

                logger.info(f"–§–∞–π–ª –Ω–∞–π–¥–µ–Ω. –° {from_date} –ø–æ {last_date}")

                if not is_russian_workday():
                    # logger.info("–°–µ–≥–æ–¥–Ω—è –≤—ã—Ö–æ–¥–Ω–æ–π –∏–ª–∏ –ø—Ä–∞–∑–¥–Ω–∏–∫ ‚Äî –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
                    logger.info("–ü—Ä–∞–∑–¥–Ω–∏–∫ ‚Äî –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
                    return 0

                if last_date.strftime('%d.%m.%Y') == today.strftime('%d.%m.%Y') or (
                    last_date.strftime('%d.%m.%Y') == (today - timedelta(days=1)).strftime('%d.%m.%Y') and
                    datetime.now().time() < time(14, 0)
                ):
                    logger.info("–î–∞–Ω–Ω—ã–µ —É–∂–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
                    return 0
                else:
                    logger.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ. –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
                logger.info("–í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∑–∞–Ω–æ–≤–æ.")
        else:
            logger.info(f"–§–∞–π–ª {FILENAME} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å {start_date} –ø–æ {end_date}.")

        url = f"https://cbr.ru/Queries/UniDbQuery/DownloadExcel/125022?Posted=True&From={start_date}&To={end_date}&I1=true&M1=true&M3=true&M6=true&FromDate={from_dt}&ToDate={to_dt}"
        logger.info(f"–ó–∞–ø—Ä–æ—Å –ø–æ —Å—Å—ã–ª–∫–µ: {url}")

        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return -1

        with open(FILENAME, "wb") as f:
            f.write(response.content)
            logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {FILENAME}")

        return 1

    except RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å —Å–∞–π—Ç–∞ –¶–ë: {e}")
        return -1
    except Exception as e:
        logger.exception(f"–ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        return -2


# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤, –ø–æ—Ö–æ–∂–µ –Ω–∞ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
def analitics(FILENAME="ruonia_data.xlsx"):
    today_str = datetime.today().strftime("%Y-%m-%d")
    base_filename = f"ruonia_trend_{today_str}"
    ext = ".png"

    output_dir = os.path.join(os.getcwd(), "src")
    os.makedirs(output_dir, exist_ok=True)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞
    version = 1
    output_path = os.path.join(output_dir, base_filename + ext)
    while os.path.exists(output_path):
        version += 1
        output_path = os.path.join(output_dir, f"{base_filename}_v{version}{ext}")

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_excel(FILENAME)
        df = df.rename(columns={
            "–ò–Ω–¥–µ–∫—Å": "RUONIA",
            "1 –º–µ—Å—è—Ü": "1 –º–µ—Å",
            "3 –º–µ—Å—è—Ü–∞": "3 –º–µ—Å",
            "6 –º–µ—Å—è—Ü–µ–≤": "6 –º–µ—Å"
        })
        df["–î–∞—Ç–∞"] = pd.to_datetime(df["–î–∞—Ç–∞"], dayfirst=True)
        df = df.dropna(subset=["RUONIA", "1 –º–µ—Å", "3 –º–µ—Å", "6 –º–µ—Å"])
        df = df.sort_values("–î–∞—Ç–∞")

        # --- üìà –ì—Ä–∞—Ñ–∏–∫ —Å–æ –≤—Å–µ–º–∏ –¥–∞–Ω–Ω—ã–º–∏ ---
        plt.figure(figsize=(14, 7))
        plt.plot(df["–î–∞—Ç–∞"], df["RUONIA"], label="RUONIA (overnight)", linewidth=2)
        plt.plot(df["–î–∞—Ç–∞"], df["1 –º–µ—Å"], label="RUONIA 1 –º–µ—Å", linestyle="--")
        plt.plot(df["–î–∞—Ç–∞"], df["3 –º–µ—Å"], label="RUONIA 3 –º–µ—Å", linestyle="-.")
        plt.plot(df["–î–∞—Ç–∞"], df["6 –º–µ—Å"], label="RUONIA 6 –º–µ—Å", linestyle=":")

        plt.title(f"–î–∏–Ω–∞–º–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ RUONIA –∏ —Å—Ä–æ—á–Ω—ã—Ö —Å—Ç–∞–≤–æ–∫ –¥–æ {today_str}", fontsize=14)
        plt.xlabel("–î–∞—Ç–∞")
        plt.ylabel("–°—Ç–∞–≤–∫–∞ (%)")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(output_path)
        plt.close()

        logger.info(f"üìà –ì—Ä–∞—Ñ–∏–∫ (–≤—Å–µ –¥–∞–Ω–Ω—ã–µ) —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

        # --- üìâ –ì—Ä–∞—Ñ–∏–∫ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 90 –¥–Ω–µ–π ---
        short_df = df[df["–î–∞—Ç–∞"] >= (datetime.today() - timedelta(days=90))]

        plt.figure(figsize=(14, 7))
        plt.plot(short_df["–î–∞—Ç–∞"], short_df["1 –º–µ—Å"], label="RUONIA 1 –º–µ—Å", linestyle="--")
        plt.plot(short_df["–î–∞—Ç–∞"], short_df["3 –º–µ—Å"], label="RUONIA 3 –º–µ—Å", linestyle="-.")
        plt.plot(short_df["–î–∞—Ç–∞"], short_df["6 –º–µ—Å"], label="RUONIA 6 –º–µ—Å", linestyle=":")

        plt.title(f"RUONIA (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 90 –¥–Ω–µ–π) –¥–æ {today_str}", fontsize=14)
        plt.xlabel("–î–∞—Ç–∞")
        plt.ylabel("–°—Ç–∞–≤–∫–∞ (%)")
        plt.legend()
        plt.grid(True)

        ax = plt.gca()
        ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y'))
        plt.xticks(rotation=90)

        plt.tight_layout()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ç–æ—Ä–æ–π —Ñ–∞–π–ª —Å _last90
        short_filename = f"{base_filename}_last90"
        short_output_path = os.path.join(output_dir, short_filename + ext)
        version = 1
        while os.path.exists(short_output_path):
            version += 1
            short_output_path = os.path.join(output_dir, f"{short_filename}_v{version}{ext}")

        plt.savefig(short_output_path)
        plt.close()

        logger.info(f"üìâ –ì—Ä–∞—Ñ–∏–∫ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 90 –¥–Ω–µ–π) —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {short_output_path}")
        return output_path, short_output_path

    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}")
        return None
    
#–ø—Ä–æ–≤–µ—Ç—Å–∏ –∞–Ω–∞–ª–∏–∑ –†–£–û–ù–ò–ò 
def make_analyze_ruonia(filepath="ruonia_data.xlsx"):
    try:
        df = pd.read_excel(filepath)
        df = df.rename(columns={
            "–ò–Ω–¥–µ–∫—Å": "RUONIA",
            "1 –º–µ—Å—è—Ü": "1 –º–µ—Å",
            "3 –º–µ—Å—è—Ü–∞": "3 –º–µ—Å",
            "6 –º–µ—Å—è—Ü–µ–≤": "6 –º–µ—Å"
        })
        df["–î–∞—Ç–∞"] = pd.to_datetime(df["–î–∞—Ç–∞"], dayfirst=True)
        df = df.sort_values("–î–∞—Ç–∞")

        last_30 = df.tail(30)
        last_15 = last_30.tail(15)
        last_10 = last_30.tail(10)

        latest_date = last_10["–î–∞—Ç–∞"].iloc[-1].strftime("%d.%m.%Y")
        previous_date = last_10["–î–∞—Ç–∞"].iloc[-2].strftime("%d.%m.%Y")

        indicators = ["RUONIA", "1 –º–µ—Å", "3 –º–µ—Å", "6 –º–µ—Å"]
        full_text = f"üìÖ –ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö: {latest_date}\n"

        for col in indicators:
            latest = last_10[col].iloc[-1]
            previous = last_10[col].iloc[-2]

            delta_1 = latest - previous
            delta_10 = last_10[col].iloc[-1] - last_10[col].iloc[0]
            delta_15 = last_15[col].iloc[-1] - last_15[col].iloc[0]
            delta_30 = last_30[col].iloc[-1] - last_30[col].iloc[0]

            mean_10 = last_10[col].mean()
            mean_15 = last_15[col].mean()
            mean_30 = last_30[col].mean()

            if delta_10 > 0 and delta_15 > 0 and delta_30 > 0:
                trend = "üìà –ø–ª–∞–≤–Ω—ã–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥"
            elif delta_10 < 0 and delta_15 < 0 and delta_30 < 0:
                trend = "üìâ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ"
            else:
                trend = "üìä –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ"

            full_text += (
                f"\nüìå **{col}**\n"
                f"‚Ä¢ –°–µ–≥–æ–¥–Ω—è: {latest:.4f}\n"
                f"‚Ä¢ –í—á–µ—Ä–∞ ({previous_date}): {previous:.4f}\n"
                f"‚Ä¢ Œî –∑–∞ –¥–µ–Ω—å: {delta_1:+.4f}\n"
                f"‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∑–∞ 10 –¥–Ω–µ–π: {mean_10:.4f}\n"
                f"‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∑–∞ 15 –¥–Ω–µ–π: {mean_15:.4f}\n"
                f"‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∑–∞ 30 –¥–Ω–µ–π: {mean_30:.4f}\n"
                f"‚Ä¢ –†–æ—Å—Ç –∑–∞ 10 –¥–Ω–µ–π: {delta_10:+.4f}\n"
                f"‚Ä¢ –†–æ—Å—Ç –∑–∞ 15 –¥–Ω–µ–π: {delta_15:+.4f}\n"
                f"‚Ä¢ –†–æ—Å—Ç –∑–∞ 30 –¥–Ω–µ–π: {delta_30:+.4f}\n"
                f"‚Ä¢ –¢—Ä–µ–Ω–¥: {trend}\n"
            )

        logger.info("üßæ –ê–Ω–∞–ª–∏–∑ RUONIA —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω.")
        logger.debug(f"\n{full_text}")
        return full_text

    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–Ω–∞–ª–∏—Ç–∏–∫–µ RUONIA: {e}")
        return None


def send_info_ruonia(client, recipients):
    folder_path = os.path.join(os.getcwd(), "src")
    base_name = "ruonia_trend_"
    short_base_name = "ruonia_trend_"
    short_suffix = "_last90"
    extension = ".png"

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ–∞–π–ª–æ–≤
    matching_files = [
        f for f in os.listdir(folder_path)
        if f.startswith(base_name) and f.endswith(extension) and short_suffix not in f
    ] if os.path.exists(folder_path) else []

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ (last90)
    matching_short_files = [
        f for f in os.listdir(folder_path)
        if f.startswith(short_base_name) and short_suffix in f and f.endswith(extension)
    ] if os.path.exists(folder_path) else []

    # if matching_files:
    #     matching_files.sort(reverse=True)
    #     latest_file = os.path.join(folder_path, matching_files[0])
    #     logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥—Ä–∞—Ñ–∏–∫: {latest_file}")
    # else:
    #     logger.warning("üìÇ –ì—Ä–∞—Ñ–∏–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –ø–æ–º–æ—â—å—é analitics()...")
    #     latest_file = analitics()

    #–ó–∞–º–µ–Ω–∏–ª –Ω–∞ –≤—Å–µ–≥–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
    logger.warning("üìÇ –í—Å–µ–≥–¥–∞!!!!. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –ø–æ–º–æ—â—å—é analitics()...")
    # latest_file, latest_short_file = analitics()

    # latest_file = analitics()
    result = analitics()
    if not result:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏.")

    latest_file, latest_short_file = result

    if not os.path.exists(latest_file):
        logger.error("‚ùå –§–∞–π–ª –≥—Ä–∞—Ñ–∏–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")


    # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ short-—Ñ–∞–π–ª–∞
    latest_short_file = None
    if matching_short_files:
        matching_short_files.sort(reverse=True)
        latest_short_file = os.path.join(folder_path, matching_short_files[0])
        logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω –∫–æ—Ä–æ—Ç–∫–∏–π –≥—Ä–∞—Ñ–∏–∫ (90 –¥–Ω–µ–π): {latest_short_file}")





    if not latest_file or not os.path.exists(latest_file):
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª –≥—Ä–∞—Ñ–∏–∫–∞ RUONIA.")
        return

    analysis = make_analyze_ruonia()

    for chat_id in recipients:
        try:
            logger.info(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –≥—Ä–∞—Ñ–∏–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –≤ —á–∞—Ç: {chat_id}")
            client.send_photo(
                chat_id,
                photo=latest_file,
                caption="üìà –ì—Ä–∞—Ñ–∏–∫ RUONIA –∑–∞ –≤—Å—ë –≤—Ä–µ–º—è –¥–æ " + datetime.today().strftime("%Y-%m-%d")
            )

            # –û—Ç–ø—Ä–∞–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ (last90), –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω
            if latest_short_file and os.path.exists(latest_short_file):
                client.send_photo(
                    chat_id,
                    photo=latest_short_file,
                    caption="üìâ RUONIA –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 90 –¥–Ω–µ–π"
                )

            if analysis:
                client.send_message(chat_id, analysis)
                logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ {chat_id}")
            else:
                logger.warning(f"‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω ‚Äî —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ {chat_id}")
        except Exception as e:
            logger.exception(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –≤ {chat_id}: {e}")


# https://cbr.ru/Queries/UniDbQuery/DownloadExcel/125022?Posted=True&From=11.01.2010&To=30.04.2025&I1=true&M1=true&M3=true&M6=true&FromDate=01%2F11%2F2010&ToDate=04%2F30%2F2025

###########################################################################################################################AI
from functions.ai import run_brief

# ---------- –æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä–æ–º ----------
def send_ai(client, recipients):
    """
    –ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ (Markdown) —á–µ—Ä–µ–∑ functions.ai.run_brief –∏ —Ä–∞—Å—Å—ã–ª–∞–µ—Ç –µ–≥–æ:
      ‚Ä¢ –°–ù–ê–ß–ê–õ–ê —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏ (–ø–æ –¥–≤–µ –≥–ª–∞–≤—ã –≤ –æ–¥–Ω–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏, —É–∫–ª–∞–¥—ã–≤–∞—è—Å—å –≤ –ª–∏–º–∏—Ç Telegram),
      ‚Ä¢ –∑–∞—Ç–µ–º –ø—Ä–∏–∫–ª–∞–¥—ã–≤–∞–µ—Ç –æ–¥–∏–Ω –∞—Ä—Ö–∏–≤–Ω—ã–π .md —Ñ–∞–π–ª –≤ –ø–∞–ø–∫—É src/ai/ –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –≤ —á–∞—Ç.

    –£—Å—Ç–æ–π—á–∏–≤–∞ –∫ —Ä–∞–∑–Ω—ã–º —Å–∏–≥–Ω–∞—Ç—É—Ä–∞–º run_brief():
      - –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å (answer),
      - –∏–ª–∏ (answer, tokens),
      - –∏–ª–∏ (answer, tokens, *anything_else).  
    –í –ª—é–±–æ–º —Å–ª—É—á–∞–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç Markdown (–∑–∞–∫—Ä—ã–≤–∞–µ—Ç ```), –¥–µ–ª–∏—Ç –Ω–∞ –≥–ª–∞–≤—ã `## N. ...`.
    –ü—Ä–∏ –æ—à–∏–±–∫–µ —Ä–∞–∑–º–µ—Ç–∫–∏ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç –æ—Ç–ø—Ä–∞–≤–∫—É –±–µ–∑ parse_mode.
    """
    import os
    import re
    from datetime import datetime
    from functions.ai import run_brief

    TELEGRAM_LIMIT = 4000

    # --- helpers -----------------------------------------------------------
    def normalize_code_fences(text: str) -> str:
        # –ü—Ä–∏–≤–æ–¥–∏–º ```md/markdown –∫ –ø—Ä–æ—Å—Ç–æ–º—É ``` –∏ –∑–∞–∫—Ä—ã–≤–∞–µ–º –Ω–µ–∑–∞–∫—Ä—ã—Ç—ã–µ
        text = re.sub(r"```\s*(markdown|md|Markdown)\s*\n", "```\n", text)
        if text.count("```") % 2 == 1:
            text = text.rstrip() + "\n\n```\n"
        return text

    def split_into_chapters(md: str):
        # –ì–ª–∞–≤–∞ = –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤–∏–¥–∞: ## 1. ...
        header_pat = re.compile(r"^##\s+\d+\.[\t ]*.*$", re.M)
        matches = list(header_pat.finditer(md))
        if not matches:
            return [md]
        parts = []
        first_start = matches[0].start()
        prologue = md[:first_start].strip("\n")
        if prologue:
            parts.append(prologue)
        for i, m in enumerate(matches):
            start = m.start()
            end = matches[i+1].start() if i+1 < len(matches) else len(md)
            parts.append(md[start:end].strip("\n"))
        return parts

    def split_hard(block: str, limit: int):
        # –ê–±–∑–∞—Ü—ã -> —Å—Ç—Ä–æ–∫–∏ -> —Å–∏–º–≤–æ–ª—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É—è ``` –≤ –∫–∞–∂–¥–æ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç–µ
        parts, cur = [], ""
        def flush():
            nonlocal cur
            if cur.strip():
                parts.append(normalize_code_fences(cur).strip())
                cur = ""
        for p in block.split("\n\n"):
            chunk = p + "\n\n"
            if len(chunk) > limit:
                for ln in chunk.splitlines(True):
                    if len(ln) > limit:
                        for s in range(0, len(ln), limit):
                            part = ln[s:s+limit]
                            if cur and len(cur)+len(part) > limit:
                                flush()
                            cur += part
                    else:
                        if cur and len(cur)+len(ln) > limit:
                            flush()
                        cur += ln
            else:
                if cur and len(cur)+len(chunk) > limit:
                    flush()
                cur += chunk
        flush()
        return parts

    def bundle_messages(chapters, limit: int):
        # –°–∫–ª–µ–∏–≤–∞–µ–º –ø–æ 2 –≥–ª–∞–≤—ã, —É–≤–∞–∂–∞—è –ª–∏–º–∏—Ç. –î–ª–∏–Ω–Ω—ã–µ –≥–ª–∞–≤—ã —Ä–µ–∂–µ–º.
        msgs = []
        i, n = 0, len(chapters)
        while i < n:
            a = normalize_code_fences(chapters[i])
            if i + 1 < n:
                b = normalize_code_fences(chapters[i+1])
                if len(a) + len(b) <= limit:
                    msgs.append((a + "\n\n" + b).strip())
                    i += 2
                    continue
            if len(a) > limit:
                msgs.extend(split_hard(a, limit))
            else:
                msgs.append(a)
            i += 1
        return msgs

    # --- get model answer --------------------------------------------------
    try:
        result = run_brief()
    except Exception as e:
        for chat_id in recipients:
            try:
                client.send_message(chat_id, f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI-brief: {e}")
            except Exception:
                pass
        return

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–¥ (answer, tokens)
    answer, tokens = None, None
    if isinstance(result, tuple):
        if len(result) >= 1:
            answer = result[0]
        if len(result) >= 2:
            tokens = result[1]
    else:
        answer = result

    if not isinstance(answer, str) or not answer.strip():
        for chat_id in recipients:
            try:
                client.send_message(chat_id, "‚ö†Ô∏è –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏.")
            except Exception:
                pass
        return

    # --- prepare text & files ---------------------------------------------
    answer = normalize_code_fences(answer)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–¥–∏–Ω .md —Ñ–∞–π–ª –¥–ª—è –∞—Ä—Ö–∏–≤–∞ –∏ –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ fallback
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_dir = os.path.join(os.getcwd(), "src", "ai")
    os.makedirs(base_dir, exist_ok=True)
    md_path = os.path.join(base_dir, f"ai_brief_{ts}.md")
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(answer)
        if tokens:
            f.write("\n\n" + str(tokens))

    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π (–æ–¥–Ω–∏–º, –µ—Å–ª–∏ —É–º–µ—â–∞–µ—Ç—Å—è)
    if len(answer) <= TELEGRAM_LIMIT:
        messages = [answer]
    else:
        chapters = split_into_chapters(answer) or [answer]
        messages = bundle_messages(chapters, TELEGRAM_LIMIT)

    # --- send: messages first, then file ----------------------------------
    for chat_id in recipients:
        # 1) –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º Markdown
        sent_msgs = True
        try:
            for i, msg in enumerate(messages, 1):
                prefix = f"–ß–∞—Å—Ç—å {i}/{len(messages)}\n\n" if len(messages) > 1 else ""
                client.send_message(chat_id, prefix + msg, parse_mode="Markdown")
        except Exception:
            sent_msgs = False

        # 2) –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å ‚Äî –æ—Ç–ø—Ä–∞–≤–∏–º –±–µ–∑ parse_mode
        if not sent_msgs:
            try:
                for i, msg in enumerate(messages, 1):
                    prefix = f"–ß–∞—Å—Ç—å {i}/{len(messages)}\n\n" if len(messages) > 1 else ""
                    client.send_message(chat_id, prefix + msg)
                sent_msgs = True
            except Exception:
                sent_msgs = False

        # 3) –ö–æ—Ä–æ—Ç–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø—Ä–æ —Ç–æ–∫–µ–Ω—ã
        if tokens and sent_msgs:
            try:
                client.send_message(chat_id, str(tokens))
            except Exception:
                pass

        # 4) –ò –æ–¥–∏–Ω –∞—Ä—Ö–∏–≤–Ω—ã–π .md —Ñ–∞–π–ª
        try:
            client.send_document(chat_id, md_path, caption="üìÑ AI-brief (.md)")
        except Exception:
            pass



###########################################################################################################################AI


#################################### –í–µ—Ä–Ω—É—Ç—å ######
# check_if_need_new_rec()

# analitics()  # –õ–∏–±–æ –ø–µ—Ä–µ–¥–µ–ª–∞—Ç—å 
#################################### –í–µ—Ä–Ω—É—Ç—å ######


###### –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è ####
# import subprocess
# import os
# import sys

# def check_git_update(commit_file="log/current_commit.txt"):
#     try:
#         # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–∞–ø–∫–∞ log —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
#         os.makedirs(os.path.dirname(commit_file), exist_ok=True)

#         # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –∫–æ–º–º–∏—Ç —Å origin
#         subprocess.run(["git", "fetch"], check=True)
#         new_commit = subprocess.check_output(
#             ["git", "rev-parse", "origin/main"], text=True
#         ).strip()

#         # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞—ë–º –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π –∫–æ–º–º–∏—Ç
#         if not os.path.exists(commit_file):
#             with open(commit_file, "w") as f:
#                 f.write(new_commit)
#             logger.info(f"üìÑ –§–∞–π–ª {commit_file} —Å–æ–∑–¥–∞–Ω. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∫–æ–º–º–∏—Ç: {new_commit}")
#             return None  # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ ‚Äî –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è

#         # –°—á–∏—Ç—ã–≤–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –∫–æ–º–º–∏—Ç
#         with open(commit_file, "r") as f:
#             last_commit = f.read().strip()

#         if new_commit != last_commit:
#             logger.info(f"üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–æ–≤—ã–π –∫–æ–º–º–∏—Ç: {new_commit}")
#             return new_commit
#         else:
#             logger.info("‚úÖ –í–µ—Ä—Å–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–∞. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
#             return None

#     except Exception as e:
#         logger.exception("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Git:")
#         return None


# def update_and_restart(new_commit, commit_file="log/current_commit.txt"):
#     try:
#         subprocess.run(["git", "pull"], check=True)

#         with open(commit_file, "w") as f:
#             f.write(new_commit)

#         logger.info("‚ôªÔ∏è –ü—Ä–æ–µ–∫—Ç –æ–±–Ω–æ–≤–ª—ë–Ω. –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º...")
#         os.execv(sys.executable, ['python'] + sys.argv)

#     except Exception as e:
#         logger.exception("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ:")



# commit_file = "log/current_commit.txt"
# new_commit = check_git_update(commit_file)
# if new_commit:
#     update_and_restart(new_commit, commit_file)



################################################################################–í–ï–†–ù–£–¢–¨
from functions.auto_update import check_and_restart_if_updated
check_and_restart_if_updated()
#################################################################################–í–ï–†–ù–£–¢–¨
# ######

load_dotenv()  

api_hash = os.getenv('api_hash')
for_whom = os.getenv('for_whom')
api_id = os.getenv('api_id')
bot_token = os.getenv('bot_token')

recipients_raw = os.getenv("for_whom_list", "")
recipients = [r.strip() for r in recipients_raw.split(",") if r.strip()]
if not recipients:
    raise ValueError("‚ùå –ù–µ—Ç –ø–æ–ª—É—á–∞—Ç–µ–ª–µ–π. –£–±–µ–¥–∏—Å—å, —á—Ç–æ for_whom_list –∑–∞–¥–∞–Ω –≤ .env")




from pyrogram import Client, idle

#################  –≤–µ—Ä–Ω—É—Ç—å 
client = Client(name='me_client', api_id=api_id, api_hash=api_hash, bot_token = bot_token )
# –ó–∞–ø—É—Å–∫ –∫–ª–∏–µ–Ω—Ç–∞
client.start()

        


check_if_need_new_rec()
send_info_ruonia(client, recipients)

time.sleep(10)
send_wordstat_digest(client, recipients)

time.sleep(10)
send_ai(client, recipients)


# idle()

# –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏
client.stop()

#################### –≤–µ—Ä–Ω—É—Ç—å 







