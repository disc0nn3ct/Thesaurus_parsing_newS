# ThesaurusV2

import pandas as pd
from datetime import datetime, timedelta, time
import os
import requests
from requests.exceptions import RequestException
import matplotlib.pyplot as plt
from dotenv import load_dotenv
import logging
import matplotlib.dates as mdates
import re

import sys


import holidays
from datetime import date

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
log_dir = os.path.join(os.getcwd(), "log")
os.makedirs(log_dir, exist_ok=True)

# –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ –ª–æ–≥-—Ñ–∞–π–ª—É
log_file = os.path.join(log_dir, "ruonia_log.txt")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
logging.basicConfig(
    filename=log_file,
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    encoding="utf-8"
)

logger = logging.getLogger(__name__)


def is_russian_workday(check_date=None):
    if check_date is None:
        check_date = date.today()

    ru_holidays = holidays.Russia()




    return check_date not in ru_holidays # –≤—ã—Ö–æ–¥–Ω—ã–µ —Å–± –≤—Å –∏    
    # return check_date.weekday() < 5 and check_date not in ru_holidays # –≤—ã—Ö–æ–¥–Ω—ã–µ —Å–± –≤—Å –∏

#### 
# ======================= WORDSTAT: –∫—Ä–∏–∑–∏—Å–Ω—ã–µ –∫–ª—é—á–∏ -> –≥—Ä–∞—Ñ–∏–∫–∏ -> —Ä–∞—Å—Å—ã–ª–∫–∞ =======================
import os
import re
import json
import time
import requests
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta, date
from dateutil.relativedelta import relativedelta
from dotenv import load_dotenv

import numpy as np
from sklearn.linear_model import Ridge

load_dotenv()

# –∏–∑ .env:
#  - WORDSTAT_OAUTH  ‚Äî OAuth-—Ç–æ–∫–µ–Ω, –∫–æ—Ç–æ—Ä—ã–π —Ç—ã –ø–æ–ª—É—á–∏–ª –ø–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ ...
#  - WORDSTAT_CLIENT_ID ‚Äî ClientId –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è (—Å —Ç–æ–π –∂–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
WORDSTAT_OAUTH = os.getenv("WORDSTAT_OAUTH")
WORDSTAT_CLIENT_ID = os.getenv("WORDSTAT_CLIENT_ID")

# –±–∞–∑–æ–≤—ã–π URL API –∏–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –¥–æ–∫–∏
WORDSTAT_API = "https://api.wordstat.yandex.net"
WORDSTAT_REGION_RUSSIA = 225  # –†–æ—Å—Å–∏—è
WORDSTAT_SAVE_DIR = os.path.join("src", "wordstat")
WORDSTAT_RAW_DIR = os.path.join(WORDSTAT_SAVE_DIR, "raw")  # —Å—é–¥–∞ –±—É–¥–µ–º —Å–∫–ª–∞–¥—ã–≤–∞—Ç—å –≤—Å–µ –æ—Ç–≤–µ—Ç—ã API
WORDSTAT_MAPS_DIR = os.path.join(WORDSTAT_SAVE_DIR, "maps")
os.makedirs(WORDSTAT_MAPS_DIR, exist_ok=True)

os.makedirs(WORDSTAT_SAVE_DIR, exist_ok=True)
os.makedirs(WORDSTAT_RAW_DIR, exist_ok=True)
111
# –∫–ª–∏–µ–Ω—Ç Perplexity –∏–∑ —Ç–≤–æ–µ–≥–æ ai.py
from functions.ai import client as ai_client
from functions.wordstat_api import _wordstat_post

import hashlib

def slugify_phrase(s: str, max_len: int = 60) -> str:
    """
    –î–µ–ª–∞–µ—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–π slug, —Å–æ—Ö—Ä–∞–Ω—è—è –∫–∏—Ä–∏–ª–ª–∏—Ü—É (—á–µ—Ä–µ–∑ \w —Å UNICODE),
    –ø–ª—é—Å –¥–æ–±–∞–≤–ª—è–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π —Ö—ç—à –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏.
    """
    s = (s or "").strip().lower()
    base = re.sub(r"[^\w.-]+", "_", s, flags=re.UNICODE).strip("_")
    if not base:
        base = "phrase"
    if len(base) > max_len:
        base = base[:max_len].rstrip("_")
    h = hashlib.md5(s.encode("utf-8")).hexdigest()[:8]
    return f"{base}_{h}"

def _series_metrics(y: np.ndarray, window_short: int = 8, window_long: int = 52) -> dict:
    """
    –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Ä–µ–º—è–Ω–∫–µ:
      - last, mean_long, std_long
      - z_last (–Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∏–π —É—Ä–æ–≤–µ–Ω—å –∞–Ω–æ–º–∞–ª—å–Ω—ã–π)
      - mom_short: –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Å—Ä–µ–¥–Ω–µ–≥–æ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ
      - vol_short: –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –∫–æ—Ä–æ—Ç–∫–æ–º –æ–∫–Ω–µ
    """
    y = np.asarray(y, dtype=float)
    y = y[~np.isnan(y)]
    if len(y) < 10:
        return {}

    wL = min(window_long, len(y))
    wS = min(window_short, len(y))

    last = float(y[-1])
    mean_long = float(np.mean(y[-wL:]))
    std_long = float(np.std(y[-wL:]) + 1e-9)

    mean_short = float(np.mean(y[-wS:]))
    z_last = float((last - mean_long) / std_long)
    mom_short = float((mean_short - mean_long) / (mean_long + 1e-9))  # –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ
    vol_short = float(np.std(y[-wS:]) / (mean_short + 1e-9))          # –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å

    return {
        "last": last,
        "mean_long": mean_long,
        "std_long": std_long,
        "z_last": z_last,
        "mom_short": mom_short,
        "vol_short": vol_short,
    }


def _forecast_ridge_log1p(y: np.ndarray, horizon: int = 4, train_weeks: int = 156) -> dict:
    """
    –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ horizon –Ω–µ–¥–µ–ª—å:
      - ridge —Ä–µ–≥—Ä–µ—Å—Å–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ log1p(y)
      - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã (yhat) –∏ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ—Ä–∏–¥–æ—Ä (–ø—Ä–∏–º–µ—Ä–Ω—ã–π) –∏–∑ std –æ—Å—Ç–∞—Ç–∫–æ–≤
    """
    y = np.asarray(y, dtype=float)
    y = y[~np.isnan(y)]
    n = len(y)
    if n < 30:
        return {"yhat": [], "lo": [], "hi": []}

    # –æ–≥—Ä–∞–Ω–∏—á–∏–º –æ–±—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ train_weeks (–ø—Ä–∏–º–µ—Ä–Ω–æ 3 –≥–æ–¥–∞)
    start = max(0, n - train_weeks)
    y_train = y[start:]
    t = np.arange(len(y_train)).reshape(-1, 1)

    # log1p –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ –ø–∏–∫–∞–º
    y_log = np.log1p(np.clip(y_train, 0, None))

    model = Ridge(alpha=1.0)
    model.fit(t, y_log)

    # –æ—Å—Ç–∞—Ç–∫–∏ -> –æ—Ü–µ–Ω–∫–∞ —à—É–º–∞
    y_log_pred = model.predict(t)
    resid = y_log - y_log_pred
    resid_std = float(np.std(resid) + 1e-9)

    # –ø—Ä–æ–≥–Ω–æ–∑
    t_future = np.arange(len(y_train), len(y_train) + horizon).reshape(-1, 1)
    fut_log = model.predict(t_future)

    # –≥—Ä—É–±—ã–π 80% –∫–æ—Ä–∏–¥–æ—Ä (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ 95%)
    z = 1.28
    lo_log = fut_log - z * resid_std
    hi_log = fut_log + z * resid_std

    yhat = np.expm1(fut_log)
    lo = np.expm1(lo_log)
    hi = np.expm1(hi_log)

    # –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ
    yhat = np.clip(yhat, 0, None)
    lo = np.clip(lo, 0, None)
    hi = np.clip(hi, 0, None)

    return {
        "yhat": [float(v) for v in yhat],
        "lo": [float(v) for v in lo],
        "hi": [float(v) for v in hi],
        "resid_std_log": resid_std,
    }


def interpret_signal(metrics: dict) -> str:
    """
    –ö–æ—Ä–æ—Ç–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º.
    """
    if not metrics:
        return "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —É–≤–µ—Ä–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏."

    z = metrics["z_last"]
    mom = metrics["mom_short"]
    vol = metrics["vol_short"]

    parts = []

    # —É—Ä–æ–≤–µ–Ω—å
    if z >= 2.0:
        parts.append("–∞–Ω–æ–º–∞–ª—å–Ω–æ –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–∞ (–≤–æ–∑–º–æ–∂–Ω–∞ –ø–∞–Ω–∏–∫–∞/—Å–æ–±—ã—Ç–∏–µ)")
    elif z <= -2.0:
        parts.append("–∞–Ω–æ–º–∞–ª—å–Ω–æ –Ω–∏–∑–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–∞ (–∑–∞—Ç—É—Ö–∞–Ω–∏–µ)")
    elif z >= 1.0:
        parts.append("–≤—ã—à–µ –æ–±—ã—á–Ω–æ–≥–æ")
    elif z <= -1.0:
        parts.append("–Ω–∏–∂–µ –æ–±—ã—á–Ω–æ–≥–æ")
    else:
        parts.append("–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã")

    # –∏–º–ø—É–ª—å—Å
    if mom >= 0.25:
        parts.append("—É—Å—Ç–æ–π—á–∏–≤—ã–π —Ä–æ—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–µ–¥–µ–ª—å")
    elif mom <= -0.25:
        parts.append("—É—Å—Ç–æ–π—á–∏–≤–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –Ω–µ–¥–µ–ª—å")

    # –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
    if vol >= 0.35:
        parts.append("–≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (–Ω–æ–≤–æ—Å—Ç–Ω–æ–π —à—É–º/—Ä–µ–∑–∫–∏–µ –≤—Å–ø–ª–µ—Å–∫–∏)")

    return "; ".join(parts).capitalize() + "."


def make_phrase_report(phrase: str, df: pd.DataFrame, horizon: int = 4) -> dict:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ + –ø—Ä–æ–≥–Ω–æ–∑–æ–º + —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π.
    """
    y = df["count"].astype(float).to_numpy()
    metrics = _series_metrics(y)
    fc = _forecast_ridge_log1p(y, horizon=horizon)

    # –±–∞–∑–æ–≤—ã–µ —á–∏—Å–ª–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞
    yhat = fc.get("yhat", [])
    lo = fc.get("lo", [])
    hi = fc.get("hi", [])

    report = {
        "phrase": phrase,
        "metrics": metrics,
        "forecast": fc,
        "interpretation": interpret_signal(metrics),
    }

    # —É–¥–æ–±–Ω—ã–µ –ø–æ–ª—è
    if yhat:
        report["forecast_next"] = yhat[0]
        report["forecast_horizon"] = yhat
        report["forecast_ci_next"] = (lo[0], hi[0]) if lo and hi else None
    else:
        report["forecast_next"] = None
        report["forecast_horizon"] = []

    return report


def build_and_save_charts(df_all: pd.DataFrame, out_dir: str = WORDSTAT_SAVE_DIR) -> tuple[list[str], list[dict]]:
    """
    –ü–æ –∫–∞–∂–¥–æ–π —Ñ—Ä–∞–∑–µ —Ä–∏—Å—É–µ–º PNG: —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è + –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 4 –Ω–µ–¥–µ–ª–∏.
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º (files, reports).
    """
    saved = []
    reports = []

    for phrase, df in df_all.groupby("phrase"):
        if df.empty:
            continue

        df = df.sort_values("date").reset_index(drop=True)
        rep = make_phrase_report(phrase, df, horizon=4)
        reports.append(rep)

        # –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞ –∫ —Ä–∏—Å–æ–≤–∞–Ω–∏—é
        yhat = rep.get("forecast_horizon", [])
        lohi = rep.get("forecast", {})
        lo = lohi.get("lo", [])
        hi = lohi.get("hi", [])

        last_date = df["date"].iloc[-1]
        future_dates = [last_date + pd.Timedelta(days=7*(i+1)) for i in range(len(yhat))]

        plt.figure(figsize=(12, 5))
        plt.plot(df["date"], df["count"], linewidth=2, label="–§–∞–∫—Ç")

        if yhat:
            plt.plot(future_dates, yhat, linewidth=2, linestyle="--", label="–ü—Ä–æ–≥–Ω–æ–∑ (4 –Ω–µ–¥.)")
            # –∫–æ—Ä–∏–¥–æ—Ä
            if lo and hi:
                plt.fill_between(future_dates, lo, hi, alpha=0.2, label="–ö–æ—Ä–∏–¥–æ—Ä (‚âà80%)")

        title = f"Wordstat weekly: {phrase}"
        plt.title(title)
        plt.xlabel("–î–∞—Ç–∞ (–Ω–µ–¥–µ–ª–∏)")
        plt.ylabel("–ß–∏—Å–ª–æ –∑–∞–ø—Ä–æ—Å–æ–≤")
        plt.grid(True)
        plt.legend()
        plt.tight_layout()

        # safe_phrase = re.sub(r"[^a-zA-Z0-9_.-]+", "_", phrase.strip())
        # fname = f"wordstat_{safe_phrase}.png"
        safe_phrase = slugify_phrase(phrase)
        fname = f"wordstat_{safe_phrase}.png"
        fpath = os.path.join(out_dir, fname)
        base, ext = os.path.splitext(fpath)
        k = 1
        while os.path.exists(fpath):
            k += 1
            fpath = f"{base}_{k}{ext}"

        plt.savefig(fpath)
        plt.close()
        saved.append(fpath)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—ë–Ω –≥—Ä–∞—Ñ–∏–∫: {fpath}")

    return saved, reports


def _last_sunday_on_or_before(d: date) -> date:
    # Monday=0..Sunday=6 -> –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ –ø—Ä–æ—à–µ–¥—à–µ–≥–æ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å—è
    return d - timedelta(days=(d.weekday() + 1) % 7)


def _first_monday_on_or_after(d: date) -> date:
    return d + timedelta(days=(7 - d.weekday()) % 7)


def _prepare_week_bounds():
    """
    fromDate: –ø–µ—Ä–≤—ã–π –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫ >= 2018-01-01
    toDate: –ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ –Ω–µ –ø–æ–∑–¥–Ω–µ–µ —Å–µ–≥–æ–¥–Ω—è-2
    """
    today = datetime.today().date()
    to_date = today - timedelta(days=2)
    to_date = _last_sunday_on_or_before(to_date)

    from_date = date(2018, 1, 1)
    from_date = _first_monday_on_or_after(from_date)
    return from_date, to_date


from functions.regions_digest import send_incidents_daily_regions_digest






def plot_daily_heatmap(df: pd.DataFrame, phrase: str, out_path: str, top_regions: int = 25):
    """
    Heatmap: —Å—Ç—Ä–æ–∫–∏=regionId, —Å—Ç–æ–ª–±—Ü—ã=–¥–∞—Ç—ã, –∑–Ω–∞—á–µ–Ω–∏–µ=count.
    """
    sdf = df[df["phrase"] == phrase].copy()
    if sdf.empty:
        logger.warning(f"plot_daily_heatmap: –ø—É—Å—Ç–æ –¥–ª—è '{phrase}'")
        return None

    # —É–ø–æ—Ä—è–¥–æ—á–∏–º —Ä–µ–≥–∏–æ–Ω—ã –ø–æ —Å—É–º–º–µ –∏–Ω—Ç–µ—Ä–µ—Å–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥
    order = (sdf.groupby("regionId")["count"].sum()
               .sort_values(ascending=False)
               .head(top_regions)
               .index.tolist())
    sdf = sdf[sdf["regionId"].isin(order)]

    pivot = (sdf.pivot_table(index="regionId", columns="date", values="count", aggfunc="sum")
               .fillna(0))

    plt.figure(figsize=(14, max(6, 0.28 * len(pivot.index))))
    plt.imshow(pivot.values, aspect="auto")
    plt.title(f"Wordstat daily heatmap (top {top_regions} regions): {phrase}")
    plt.xlabel("–î–∞—Ç–∞")
    plt.ylabel("–†–µ–≥–∏–æ–Ω (regionId)")

    cols = list(pivot.columns)
    step = max(1, len(cols) // 10)
    xticks = list(range(0, len(cols), step))
    plt.xticks(xticks, [pd.to_datetime(cols[i]).strftime("%m-%d") for i in xticks], rotation=45)

    plt.yticks(range(len(pivot.index)), [str(x) for x in pivot.index])
    plt.colorbar(label="count")
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()
    logger.info(f"üíæ heatmap —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {out_path}")
    return out_path



def get_crisis_keywords_via_perplexity() -> list[str]:
    """
    –ë–µ—Ä—ë–º 8 —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ –ø—Ä–æ –∫—Ä–∏–∑–∏—Å—ã/–ø—Ä–æ–±–ª–µ–º—ã/—Å—Ç—Ä–µ—Å—Å—ã –Ω–∞—Å–µ–ª–µ–Ω–∏—è
    —á–µ—Ä–µ–∑ —Ç–æ–≥–æ –∂–µ –∫–ª–∏–µ–Ω—Ç–∞, —á—Ç–æ –∏ run_brief() (ai.py).

    –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ ‚Äî JSON-–º–∞—Å—Å–∏–≤ —Å—Ç—Ä–æ–∫ (—Ä–æ–≤–Ω–æ 8).
    """
    sys_prompt = (
    "–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –ò–ò-–∞–≥–µ–Ω—Ç, —Ñ–æ—Ä–º–∏—Ä—É—é—â–∏–π –ø–æ–∏—Å–∫–æ–≤—ã–µ –ò–ù–î–ò–ö–ê–¢–û–†–´ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π —Ç—Ä–µ–≤–æ–≥–∏ "
    "–∏ —ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∏–Ω–∞–º–∏–∫–∏ –≤ –Ø–Ω–¥–µ–∫—Å –í–æ—Ä–¥—Å—Ç–∞—Ç–µ. "
    "–°—Ñ–æ—Ä–º–∏—Ä—É–π —Ä–æ–≤–Ω–æ 8 —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ª—é–¥–∏ –†–ï–ê–õ–¨–ù–û –≤–≤–æ–¥—è—Ç "
    "–≤ –Ø–Ω–¥–µ–∫—Å –∏–ª–∏ Google, –∏ –ø–æ –¥–∏–Ω–∞–º–∏–∫–µ –∫–æ—Ç–æ—Ä—ã—Ö (—Ä–æ—Å—Ç, –ø–∏–∫–∏, –∑–∞—Ç—É—Ö–∞–Ω–∏–µ) –º–æ–∂–Ω–æ –¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã "
    "–æ –∫—Ä–∏–∑–∏—Å–∞—Ö, –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏, —É—Ö—É–¥—à–µ–Ω–∏–∏ –∏–ª–∏ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Å–∏—Ç—É–∞—Ü–∏–∏ –≤ —Å—Ç—Ä–∞–Ω–µ –∏ –º–∏—Ä–µ. "
    "–ó–∞–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–º–∏ (1‚Äì4 —Å–ª–æ–≤–∞), –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Ç–µ–ª–µ–≥—Ä–∞—Ñ–Ω–æ–≥–æ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –≤–≤–æ–¥–∞. "
    "–î–æ–ø—É—Å–∫–∞–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¢–û–õ–¨–ö–û –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ '+' –¥–ª—è —Ñ–∏–∫—Å–∞—Ü–∏–∏ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å–ª–æ–≤ "
    "(–Ω–∞–ø—Ä–∏–º–µ—Ä: '—Ä–∞–±–æ—Ç–∞ +–¥–æ–º–∞', '–¥–µ–Ω—å–≥–∏ +–Ω–∞ –∫–∞—Ä—Ç–µ'). "
    "–ó–∞–ø—Ä–µ—â–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã –í–æ—Ä–¥—Å—Ç–∞—Ç–∞ "
    "(-, !, –∫–∞–≤—ã—á–∫–∏, —Å–∫–æ–±–∫–∏, |). "
    "–ö–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ú–ê–ö–†–û-–ò–ù–î–ò–ö–ê–¢–û–†–û–ú –æ–¥–Ω–æ–π –∏–∑ —Å—Ñ–µ—Ä: "
    "1) —Ü–µ–Ω—ã –∏ –∏–Ω—Ñ–ª—è—Ü–∏—è, "
    "2) —Ä–∞–±–æ—Ç–∞ –∏ –¥–æ—Ö–æ–¥—ã, "
    "3) –±–∞–Ω–∫–∏ –∏ —Å–±–µ—Ä–µ–∂–µ–Ω–∏—è, "
    "4) –≤–∞–ª—é—Ç–∞ –∏ –∫—É—Ä—Å, "
    "5) –¥–æ–ª–≥–∏ –∏ –∫—Ä–µ–¥–∏—Ç—ã, "
    "6) –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±–∞–∑–æ–≤—ã—Ö —É—Å–ª—É–≥, "
    "7) –ª–µ–∫–∞—Ä—Å—Ç–≤–∞ –∏ –∑–¥–æ—Ä–æ–≤—å–µ, "
    "8) –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç—å –∏ —Å—Ç—Ä–∞—Ö –±—É–¥—É—â–µ–≥–æ. "
    "–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —É—Å—Ç–æ–π—á–∏–≤—ã–µ –º–∞—Å—Å–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã, —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–Ω–æ–≥–æ –ª–µ—Ç "
    "–∏ —Ä–µ–∞–≥–∏—Ä—É—é—â–∏–µ –Ω–∞ –∫—Ä–∏–∑–∏—Å—ã. "
    "–û—Ç–≤–µ—Ç ‚Äî –¢–û–õ–¨–ö–û JSON-–º–∞—Å—Å–∏–≤ –∏–∑ —Ä–æ–≤–Ω–æ 8 —Å—Ç—Ä–æ–∫, –±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
)

    resp = ai_client.chat.completions.create(
        model="sonar-pro",
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": "–î–∞–π –º–∞—Å—Å–∏–≤ –∏–∑ 8 —Ñ—Ä–∞–∑. –¢–æ–ª—å–∫–æ JSON, –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ/–ø–æ—Å–ª–µ."}
        ],
        temperature=0.2,
        top_p=0.9,
        max_tokens=400,
        stream=False,
    )
    raw = resp.choices[0].message.content.strip()
    try:
        arr = json.loads(raw)
        if not isinstance(arr, list) or len(arr) != 8:
            raise ValueError("–û–∂–∏–¥–∞–ª—Å—è –º–∞—Å—Å–∏–≤ –∏–∑ 8 —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        return [str(x).strip() for x in arr]
    except Exception as e:
        logger.warning(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –æ—Ç Perplexity –Ω–µ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–∏—Å—å: {e}. –ò—Å–ø–æ–ª—å–∑—É—é –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –Ω–∞–±–æ—Ä.")
        return [
            "–æ–±–≤–∞–ª", "—Ä–æ—Å—Ç —Ü–µ–Ω", "–¥–µ—Ñ–æ–ª—Ç", "–±–µ–∑—Ä–∞–±–æ—Ç–∏—Ü–∞",
            "–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞–Ω–∏–∫—É–ª—ã", "–∑–∞–∫—Ä—ã—Ç–∏–µ –±–∞–Ω–∫–æ–≤", "–¥–µ—Ñ–∏—Ü–∏—Ç –ª–µ–∫–∞—Ä—Å—Ç–≤", "–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–∞"
        ]


def fetch_wordstat_dynamics(phrase: str, regions=None, devices=None) -> pd.DataFrame:
    """
    –î–æ—Å—Ç–∞—ë—Ç weekly-–¥–∏–Ω–∞–º–∏–∫—É counts/share –ø–æ —Ñ—Ä–∞–∑–µ —Å 2018-01-01 –¥–æ —Å–µ–≥–æ–¥–Ω—è-2 (–ø–æ—Å–ª–µ–¥–Ω–µ–µ –≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ—Ç–æ–¥ /v1/dynamics.
    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç *—Å—ã—Ä–æ–π* –æ—Ç–≤–µ—Ç API –≤ JSON-—Ñ–∞–π–ª –≤ src/wordstat/raw.
    """
    if regions is None:
        regions = [WORDSTAT_REGION_RUSSIA]
    if devices is None:
        devices = ["all"]

    from_date, to_date = _prepare_week_bounds()
    payload = {
        "phrase": phrase,  # –≤ —ç—Ç–æ–º –º–µ—Ç–æ–¥–µ –¥–æ–ø—É—Å—Ç–∏–º —Ç–æ–ª—å–∫–æ –æ–ø–µ—Ä–∞—Ç–æ—Ä '+', –Ω–æ –ø—Ä–æ—Å—Ç–∞—è —Ñ—Ä–∞–∑–∞ —Ç–æ–∂–µ –æ–∫
        "period": "weekly",
        "fromDate": from_date.strftime("%Y-%m-%d"),
        "toDate": to_date.strftime("%Y-%m-%d"),
        "regions": regions,
        "devices": devices,
    }
    data = _wordstat_post("/v1/dynamics", payload)

    # === –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –≤ JSON ===
    try:
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        # safe_phrase = re.sub(r"[^a-zA-Z0-9_.-]+", "_", phrase.strip())
        # raw_fname = f"wordstat_dynamics_{safe_phrase}_{ts}.json"
        safe_phrase = slugify_phrase(phrase)
        raw_fname = f"wordstat_dynamics_{safe_phrase}_{ts}.json"

        raw_fpath = os.path.join(WORDSTAT_RAW_DIR, raw_fname)
        with open(raw_fpath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"üíæ –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç Wordstat –ø–æ '{phrase}' —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {raw_fpath}")
    except Exception as e:
        logger.exception(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—ã—Ä–æ–π –æ—Ç–≤–µ—Ç Wordstat –ø–æ '{phrase}': {e}")

    # === –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame ===
    dyn = data.get("dynamics", [])
    df = pd.DataFrame(dyn)
    if df.empty:
        df = pd.DataFrame(columns=["date", "count", "share"])
    else:
        df["date"] = pd.to_datetime(df["date"])
        df = df.sort_values("date").reset_index(drop=True)
    df["phrase"] = phrase
    return df


def send_wordstat_digest(tg_client, recipients):
    """
    –ì–ª–∞–≤–Ω—ã–π —Ä–∞–Ω–Ω–µ—Ä:
      1) –±–µ—Ä—ë–º 8 –∫–ª—é—á–µ–π —É Perplexity,
      2) —Ç—è–Ω–µ–º –¥–∏–Ω–∞–º–∏–∫—É –∏–∑ Wordstat (–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ JSON-–æ—Ç–≤–µ—Ç—ã),
      3) —Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏ (+–ø—Ä–æ–≥–Ω–æ–∑/–∞–Ω–∞–ª–∏—Ç–∏–∫–∞),
      4) —Ä–∞—Å—Å—ã–ª–∞–µ–º –≤ TG: –≥—Ä–∞—Ñ–∏–∫ + –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è + –ø—Ä–æ–≥–Ω–æ–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.

    –ü—Ä–∏ –ª—é–±–æ–π –æ—à–∏–±–∫–µ Wordstat –ø—Ä–æ—Å—Ç–æ —à–ª—ë–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, –Ω–µ –≤–∞–ª–∏–º –≤—Å—é –ø—Ä–æ–≥—Ä–∞–º–º—É.
    """
    # 1) –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
    try:
        keywords = get_crisis_keywords_via_perplexity()
    except Exception as e:
        logger.exception(f"Perplexity –Ω–µ –≤–µ—Ä–Ω—É–ª –∫–ª—é—á–∏: {e}")
        keywords = [
            "–æ–±–≤–∞–ª", "—Ä–æ—Å—Ç —Ü–µ–Ω", "–¥–µ—Ñ–æ–ª—Ç", "–±–µ–∑—Ä–∞–±–æ—Ç–∏—Ü–∞",
            "–∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –∫–∞–Ω–∏–∫—É–ª—ã", "–∑–∞–∫—Ä—ã—Ç–∏–µ –±–∞–Ω–∫–æ–≤", "–¥–µ—Ñ–∏—Ü–∏—Ç –ª–µ–∫–∞—Ä—Å—Ç–≤", "–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ —ç–ª–µ–∫—Ç—Ä–∏—á–µ—Å—Ç–≤–∞"
        ]

    logger.info(f"Wordstat: –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã: {keywords}")

    # 2) –¥–∏–Ω–∞–º–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É
    frames = []
    errors = []
    for kw in keywords:
        try:
            df_kw = fetch_wordstat_dynamics(kw)
            if not df_kw.empty:
                frames.append(df_kw)
                logger.info(f"Wordstat: –ø–æ —Ñ—Ä–∞–∑–µ '{kw}' –ø–æ–ª—É—á–µ–Ω–æ {len(df_kw)} —Ç–æ—á–µ–∫")
            else:
                logger.warning(f"Wordstat: –ø–æ —Ñ—Ä–∞–∑–µ '{kw}' –ø—Ä–∏—à—ë–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç (dynamics=[])")
            time.sleep(0.3)  # —á—É—Ç—å –ø–æ–¥–¥—Ä–æ—Å–∏–º, —á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∞–Ω–∏–ª–∏ –ø–æ RPS
        except Exception as e:
            logger.exception(f"–û—à–∏–±–∫–∞ Wordstat –ø–æ '{kw}': {e}")
            errors.append((kw, str(e)))

    # –ï—Å–ª–∏ –ù–ò –ø–æ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî —à–ª—ë–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –∏ –≤—ã—Ö–æ–¥–∏–º –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏
    if not frames:
        msg = (
            "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –í–æ—Ä–¥–∞ –Ω–∏ –ø–æ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–µ.\n"
            "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:\n"
            "‚Ä¢ –Ω–µ–≤–µ—Ä–Ω—ã–π WORDSTAT_OAUTH –∏–ª–∏ WORDSTAT_CLIENT_ID;\n"
            "‚Ä¢ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é –Ω–µ –≤—ã–¥–∞–Ω –¥–æ—Å—Ç—É–ø –∫ API –í–æ—Ä–¥–∞;\n"
            "‚Ä¢ –∏—Å—á–µ—Ä–ø–∞–Ω–∞ –∫–≤–æ—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç RPS;\n"
            "‚Ä¢ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–∏—Å–∞.\n\n"
            "–ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ —Å–º–æ—Ç—Ä–∏ –≤ –ª–æ–≥–∞—Ö (–∏—â–∏ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º 'Wordstat')."
        )
        for chat_id in recipients:
            try:
                tg_client.send_message(chat_id, msg)
            except Exception as e:
                logger.exception(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ {chat_id}: {e}")
        return

    # 3) —Å—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏ + –ø–æ–ª—É—á–∞–µ–º –æ—Ç—á—ë—Ç—ã (–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∏ –ø—Ä–æ–≥–Ω–æ–∑)
    all_df = pd.concat(frames, ignore_index=True)
    files, reports = build_and_save_charts(all_df)  # –≤–∞–∂–Ω–æ: —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å (files, reports)

    if not files:
        msg = (
            "‚ö†Ô∏è Wordstat –≤–µ—Ä–Ω—É–ª –¥–∞–Ω–Ω—ã–µ, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—Ç—Ä–æ–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ "
            "(–≤–æ–∑–º–æ–∂–Ω–æ, –≤—Å–µ DataFrame –æ–∫–∞–∑–∞–ª–∏—Å—å –ø—É—Å—Ç—ã–º–∏ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏)."
        )
        for chat_id in recipients:
            try:
                tg_client.send_message(chat_id, msg)
            except Exception as e:
                logger.exception(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ {chat_id}: {e}")
        return

    # 4) —Ä–∞—Å—Å—ã–ª–∫–∞ –≤ Telegram: –∑–∞–≥–æ–ª–æ–≤–æ–∫ + –ø–æ –∫–∞–∂–¥–æ–º—É –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—É –∫–∞—Ä—Ç–∏–Ω–∫–∞ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
    header = (
        "üìä –ï–∂–µ–Ω–µ–¥–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–í–æ—Ä–¥)\n"
        "–ü–µ—Ä–∏–æ–¥: —Å 2018-01-01 –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –Ω–µ–¥–µ–ª–∏.\n"
        "–ò—Å—Ç–æ—á–Ω–∏–∫: API /v1/dynamics.\n\n"
        "üß† –ê–Ω–∞–ª–∏—Ç–∏–∫–∞: –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –Ω–æ—Ä–º—ã (z-score), –∏–º–ø—É–ª—å—Å (8–Ω vs 52–Ω), –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 4 –Ω–µ–¥–µ–ª–∏."
    )

    for chat_id in recipients:
        try:
            tg_client.send_message(chat_id, header)

            for rep in reports:
                phrase = rep.get("phrase", "")
                metrics = rep.get("metrics", {}) or {}
                interp = rep.get("interpretation", "") or ""

                fnext = rep.get("forecast_next", None)
                fci = rep.get("forecast_ci_next", None)
                fh = rep.get("forecast_horizon", []) or []

                last = metrics.get("last", None)
                z = metrics.get("z_last", None)
                mom = metrics.get("mom_short", None)
                vol = metrics.get("vol_short", None)

                # –Ω–∞–π–¥—ë–º —Ñ–∞–π–ª –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ safe_phrase (—É—á–∏—Ç—ã–≤–∞–µ–º —Å—É—Ñ—Ñ–∏–∫—Å—ã _2 –∏ —Ç.–ø.)
                # safe_phrase = re.sub(r"[^a-zA-Z0-9_.-]+", "_", phrase.strip())
                # candidates = [fp for fp in files if f"wordstat_{safe_phrase}" in os.path.basename(fp)]
                safe_phrase = slugify_phrase(phrase)
                candidates = [fp for fp in files if f"wordstat_{safe_phrase}" in os.path.basename(fp)]
                chart_path = candidates[-1] if candidates else None

                lines = [f"üîé {phrase}"]

                if interp:
                    lines.append(f"‚Ä¢ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: {interp}")

                if last is not None:
                    lines.append(f"‚Ä¢ –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–ø–æ—Å–ª. –Ω–µ–¥–µ–ª—è): {int(round(last))}")

                if z is not None:
                    lines.append(f"‚Ä¢ –û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –Ω–æ—Ä–º—ã (z-score): {z:.2f}")

                if mom is not None:
                    lines.append(f"‚Ä¢ –ò–º–ø—É–ª—å—Å (8–Ω vs 52–Ω): {mom*100:.0f}%")

                if vol is not None:
                    lines.append(f"‚Ä¢ –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (8–Ω): {vol*100:.0f}%")

                if fnext is not None:
                    if fci and isinstance(fci, (list, tuple)) and len(fci) == 2:
                        lo, hi = fci
                        lines.append(
                            f"‚Ä¢ –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Å–ª–µ–¥. –Ω–µ–¥–µ–ª—é: {int(round(fnext))} "
                            f"(‚âà{int(round(lo))}‚Ä¶{int(round(hi))})"
                        )
                    else:
                        lines.append(f"‚Ä¢ –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —Å–ª–µ–¥. –Ω–µ–¥–µ–ª—é: {int(round(fnext))}")

                if fh:
                    fh_int = [int(round(v)) for v in fh]
                    lines.append(f"‚Ä¢ –ü—Ä–æ–≥–Ω–æ–∑ 4 –Ω–µ–¥–µ–ª–∏: {fh_int}")

                caption = "\n".join(lines)

                if chart_path:
                    tg_client.send_photo(chat_id, photo=chart_path, caption=caption)
                else:
                    tg_client.send_message(chat_id, caption)

            # –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ ‚Äî –∫–æ—Ä–æ—Ç–∫–æ —Å–æ–æ–±—â–∞–µ–º –æ–± –æ—à–∏–±–∫–∞—Ö –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–ª—é—á–∞–º
            if errors:
                err_lines = ["‚ö†Ô∏è –û—à–∏–±–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Ñ—Ä–∞–∑–∞–º:"]
                for kw, err in errors[:8]:
                    err_lines.append(f"‚Ä¢ {kw}: {err[:200]}")
                tg_client.send_message(chat_id, "\n".join(err_lines))

            logger.info(f"‚úÖ Wordstat-–¥–∞–π–¥–∂–µ—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ {chat_id}")

        except Exception as e:
            logger.exception(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ {chat_id}: {e}")





# ======================= /WORDSTAT =======================






# –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–∞ –∫–∞–∫–æ–π –ø—Ä–æ–º–µ–∂—É—Ç–µ–∫ –Ω—É–∂–µ–Ω –∑–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –Ω–∞ —Å–µ–≥–æ–¥–Ω—è, 
def check_if_need_new_rec(FILENAME="ruonia_data.xlsx"):
    try:
        today = datetime.today().date()
        end_date = today.strftime("%d.%m.%Y")
        start_date = "11.01.2010"
        from_dt = datetime.strptime(start_date, "%d.%m.%Y").strftime("%m/%d/%Y")
        to_dt = today.strftime("%m/%d/%Y")

        if os.path.exists(FILENAME):
            try:
                local_df = pd.read_excel(FILENAME)
                local_df["–î–∞—Ç–∞"] = pd.to_datetime(local_df["–î–∞—Ç–∞"], dayfirst=True)
                last_date = local_df["–î–∞—Ç–∞"].max().date()
                from_date = local_df["–î–∞—Ç–∞"].min().date()

                logger.info(f"–§–∞–π–ª –Ω–∞–π–¥–µ–Ω. –° {from_date} –ø–æ {last_date}")

                if not is_russian_workday():
                    # logger.info("–°–µ–≥–æ–¥–Ω—è –≤—ã—Ö–æ–¥–Ω–æ–π –∏–ª–∏ –ø—Ä–∞–∑–¥–Ω–∏–∫ ‚Äî –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
                    logger.info("–ü—Ä–∞–∑–¥–Ω–∏–∫ ‚Äî –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
                    return 0

                if last_date.strftime('%d.%m.%Y') == today.strftime('%d.%m.%Y') or (
                    last_date.strftime('%d.%m.%Y') == (today - timedelta(days=1)).strftime('%d.%m.%Y') and
                    datetime.now().time() < time(14, 0)
                ):
                    logger.info("–î–∞–Ω–Ω—ã–µ —É–∂–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
                    return 0
                else:
                    logger.info("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ. –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
                logger.info("–í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∑–∞–Ω–æ–≤–æ.")
        else:
            logger.info(f"–§–∞–π–ª {FILENAME} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–≥—Ä—É–∂–∞–µ–º —Å {start_date} –ø–æ {end_date}.")

        url = f"https://cbr.ru/Queries/UniDbQuery/DownloadExcel/125022?Posted=True&From={start_date}&To={end_date}&I1=true&M1=true&M3=true&M6=true&FromDate={from_dt}&ToDate={to_dt}"
        logger.info(f"–ó–∞–ø—Ä–æ—Å –ø–æ —Å—Å—ã–ª–∫–µ: {url}")

        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return -1

        with open(FILENAME, "wb") as f:
            f.write(response.content)
            logger.info(f"–§–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {FILENAME}")

        return 1

    except RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å —Å–∞–π—Ç–∞ –¶–ë: {e}")
        return -1
    except Exception as e:
        logger.exception(f"–ù–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        return -2


# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤, –ø–æ—Ö–æ–∂–µ –Ω–∞ —Å–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
def analitics(FILENAME="ruonia_data.xlsx"):
    today_str = datetime.today().strftime("%Y-%m-%d")
    base_filename = f"ruonia_trend_{today_str}"
    ext = ".png"

    output_dir = os.path.join(os.getcwd(), "src")
    os.makedirs(output_dir, exist_ok=True)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–º–µ–Ω–∏ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞
    version = 1
    output_path = os.path.join(output_dir, base_filename + ext)
    while os.path.exists(output_path):
        version += 1
        output_path = os.path.join(output_dir, f"{base_filename}_v{version}{ext}")

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        df = pd.read_excel(FILENAME)
        df = df.rename(columns={
            "–ò–Ω–¥–µ–∫—Å": "RUONIA",
            "1 –º–µ—Å—è—Ü": "1 –º–µ—Å",
            "3 –º–µ—Å—è—Ü–∞": "3 –º–µ—Å",
            "6 –º–µ—Å—è—Ü–µ–≤": "6 –º–µ—Å"
        })
        df["–î–∞—Ç–∞"] = pd.to_datetime(df["–î–∞—Ç–∞"], dayfirst=True)
        df = df.dropna(subset=["RUONIA", "1 –º–µ—Å", "3 –º–µ—Å", "6 –º–µ—Å"])
        df = df.sort_values("–î–∞—Ç–∞")

        # --- üìà –ì—Ä–∞—Ñ–∏–∫ —Å–æ –≤—Å–µ–º–∏ –¥–∞–Ω–Ω—ã–º–∏ ---
        plt.figure(figsize=(14, 7))
        plt.plot(df["–î–∞—Ç–∞"], df["RUONIA"], label="RUONIA (overnight)", linewidth=2)
        plt.plot(df["–î–∞—Ç–∞"], df["1 –º–µ—Å"], label="RUONIA 1 –º–µ—Å", linestyle="--")
        plt.plot(df["–î–∞—Ç–∞"], df["3 –º–µ—Å"], label="RUONIA 3 –º–µ—Å", linestyle="-.")
        plt.plot(df["–î–∞—Ç–∞"], df["6 –º–µ—Å"], label="RUONIA 6 –º–µ—Å", linestyle=":")

        plt.title(f"–î–∏–Ω–∞–º–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ RUONIA –∏ —Å—Ä–æ—á–Ω—ã—Ö —Å—Ç–∞–≤–æ–∫ –¥–æ {today_str}", fontsize=14)
        plt.xlabel("–î–∞—Ç–∞")
        plt.ylabel("–°—Ç–∞–≤–∫–∞ (%)")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.savefig(output_path)
        plt.close()

        logger.info(f"üìà –ì—Ä–∞—Ñ–∏–∫ (–≤—Å–µ –¥–∞–Ω–Ω—ã–µ) —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

        # --- üìâ –ì—Ä–∞—Ñ–∏–∫ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 90 –¥–Ω–µ–π ---
        short_df = df[df["–î–∞—Ç–∞"] >= (datetime.today() - timedelta(days=90))]

        plt.figure(figsize=(14, 7))
        plt.plot(short_df["–î–∞—Ç–∞"], short_df["1 –º–µ—Å"], label="RUONIA 1 –º–µ—Å", linestyle="--")
        plt.plot(short_df["–î–∞—Ç–∞"], short_df["3 –º–µ—Å"], label="RUONIA 3 –º–µ—Å", linestyle="-.")
        plt.plot(short_df["–î–∞—Ç–∞"], short_df["6 –º–µ—Å"], label="RUONIA 6 –º–µ—Å", linestyle=":")

        plt.title(f"RUONIA (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 90 –¥–Ω–µ–π) –¥–æ {today_str}", fontsize=14)
        plt.xlabel("–î–∞—Ç–∞")
        plt.ylabel("–°—Ç–∞–≤–∫–∞ (%)")
        plt.legend()
        plt.grid(True)

        ax = plt.gca()
        ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y'))
        plt.xticks(rotation=90)

        plt.tight_layout()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ç–æ—Ä–æ–π —Ñ–∞–π–ª —Å _last90
        short_filename = f"{base_filename}_last90"
        short_output_path = os.path.join(output_dir, short_filename + ext)
        version = 1
        while os.path.exists(short_output_path):
            version += 1
            short_output_path = os.path.join(output_dir, f"{short_filename}_v{version}{ext}")

        plt.savefig(short_output_path)
        plt.close()

        logger.info(f"üìâ –ì—Ä–∞—Ñ–∏–∫ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 90 –¥–Ω–µ–π) —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {short_output_path}")
        return output_path, short_output_path

    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤: {e}")
        return None
    
#–ø—Ä–æ–≤–µ—Ç—Å–∏ –∞–Ω–∞–ª–∏–∑ –†–£–û–ù–ò–ò 
def make_analyze_ruonia(filepath="ruonia_data.xlsx"):
    try:
        df = pd.read_excel(filepath)
        df = df.rename(columns={
            "–ò–Ω–¥–µ–∫—Å": "RUONIA",
            "1 –º–µ—Å—è—Ü": "1 –º–µ—Å",
            "3 –º–µ—Å—è—Ü–∞": "3 –º–µ—Å",
            "6 –º–µ—Å—è—Ü–µ–≤": "6 –º–µ—Å"
        })
        df["–î–∞—Ç–∞"] = pd.to_datetime(df["–î–∞—Ç–∞"], dayfirst=True)
        df = df.sort_values("–î–∞—Ç–∞")

        last_30 = df.tail(30)
        last_15 = last_30.tail(15)
        last_10 = last_30.tail(10)

        latest_date = last_10["–î–∞—Ç–∞"].iloc[-1].strftime("%d.%m.%Y")
        previous_date = last_10["–î–∞—Ç–∞"].iloc[-2].strftime("%d.%m.%Y")

        indicators = ["RUONIA", "1 –º–µ—Å", "3 –º–µ—Å", "6 –º–µ—Å"]
        full_text = f"üìÖ –ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö: {latest_date}\n"

        for col in indicators:
            latest = last_10[col].iloc[-1]
            previous = last_10[col].iloc[-2]

            delta_1 = latest - previous
            delta_10 = last_10[col].iloc[-1] - last_10[col].iloc[0]
            delta_15 = last_15[col].iloc[-1] - last_15[col].iloc[0]
            delta_30 = last_30[col].iloc[-1] - last_30[col].iloc[0]

            mean_10 = last_10[col].mean()
            mean_15 = last_15[col].mean()
            mean_30 = last_30[col].mean()

            if delta_10 > 0 and delta_15 > 0 and delta_30 > 0:
                trend = "üìà –ø–ª–∞–≤–Ω—ã–π –≤–æ—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥"
            elif delta_10 < 0 and delta_15 < 0 and delta_30 < 0:
                trend = "üìâ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ"
            else:
                trend = "üìä –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ"

            full_text += (
                f"\nüìå **{col}**\n"
                f"‚Ä¢ –°–µ–≥–æ–¥–Ω—è: {latest:.4f}\n"
                f"‚Ä¢ –í—á–µ—Ä–∞ ({previous_date}): {previous:.4f}\n"
                f"‚Ä¢ Œî –∑–∞ –¥–µ–Ω—å: {delta_1:+.4f}\n"
                f"‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∑–∞ 10 –¥–Ω–µ–π: {mean_10:.4f}\n"
                f"‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∑–∞ 15 –¥–Ω–µ–π: {mean_15:.4f}\n"
                f"‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∑–∞ 30 –¥–Ω–µ–π: {mean_30:.4f}\n"
                f"‚Ä¢ –†–æ—Å—Ç –∑–∞ 10 –¥–Ω–µ–π: {delta_10:+.4f}\n"
                f"‚Ä¢ –†–æ—Å—Ç –∑–∞ 15 –¥–Ω–µ–π: {delta_15:+.4f}\n"
                f"‚Ä¢ –†–æ—Å—Ç –∑–∞ 30 –¥–Ω–µ–π: {delta_30:+.4f}\n"
                f"‚Ä¢ –¢—Ä–µ–Ω–¥: {trend}\n"
            )

        logger.info("üßæ –ê–Ω–∞–ª–∏–∑ RUONIA —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω.")
        logger.debug(f"\n{full_text}")
        return full_text

    except Exception as e:
        logger.exception(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–Ω–∞–ª–∏—Ç–∏–∫–µ RUONIA: {e}")
        return None


def send_info_ruonia(client, recipients):
    folder_path = os.path.join(os.getcwd(), "src")
    base_name = "ruonia_trend_"
    short_base_name = "ruonia_trend_"
    short_suffix = "_last90"
    extension = ".png"

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ñ–∞–π–ª–æ–≤
    matching_files = [
        f for f in os.listdir(folder_path)
        if f.startswith(base_name) and f.endswith(extension) and short_suffix not in f
    ] if os.path.exists(folder_path) else []

    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ—Ä–æ—Ç–∫–∏—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤ (last90)
    matching_short_files = [
        f for f in os.listdir(folder_path)
        if f.startswith(short_base_name) and short_suffix in f and f.endswith(extension)
    ] if os.path.exists(folder_path) else []

    # if matching_files:
    #     matching_files.sort(reverse=True)
    #     latest_file = os.path.join(folder_path, matching_files[0])
    #     logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–π –≥—Ä–∞—Ñ–∏–∫: {latest_file}")
    # else:
    #     logger.warning("üìÇ –ì—Ä–∞—Ñ–∏–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –ø–æ–º–æ—â—å—é analitics()...")
    #     latest_file = analitics()

    #–ó–∞–º–µ–Ω–∏–ª –Ω–∞ –≤—Å–µ–≥–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
    logger.warning("üìÇ –í—Å–µ–≥–¥–∞!!!!. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å –ø–æ–º–æ—â—å—é analitics()...")
    # latest_file, latest_short_file = analitics()

    # latest_file = analitics()
    result = analitics()
    if not result:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏.")

    latest_file, latest_short_file = result

    if not os.path.exists(latest_file):
        logger.error("‚ùå –§–∞–π–ª –≥—Ä–∞—Ñ–∏–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")


    # –ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ short-—Ñ–∞–π–ª–∞
    latest_short_file = None
    if matching_short_files:
        matching_short_files.sort(reverse=True)
        latest_short_file = os.path.join(folder_path, matching_short_files[0])
        logger.info(f"üìÇ –ù–∞–π–¥–µ–Ω –∫–æ—Ä–æ—Ç–∫–∏–π –≥—Ä–∞—Ñ–∏–∫ (90 –¥–Ω–µ–π): {latest_short_file}")





    if not latest_file or not os.path.exists(latest_file):
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª –≥—Ä–∞—Ñ–∏–∫–∞ RUONIA.")
        return

    analysis = make_analyze_ruonia()

    for chat_id in recipients:
        try:
            logger.info(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –≥—Ä–∞—Ñ–∏–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –≤ —á–∞—Ç: {chat_id}")
            client.send_photo(
                chat_id,
                photo=latest_file,
                caption="üìà –ì—Ä–∞—Ñ–∏–∫ RUONIA –∑–∞ –≤—Å—ë –≤—Ä–µ–º—è –¥–æ " + datetime.today().strftime("%Y-%m-%d")
            )

            # –û—Ç–ø—Ä–∞–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ (last90), –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω
            if latest_short_file and os.path.exists(latest_short_file):
                client.send_photo(
                    chat_id,
                    photo=latest_short_file,
                    caption="üìâ RUONIA –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 90 –¥–Ω–µ–π"
                )

            if analysis:
                client.send_message(chat_id, analysis)
                logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ {chat_id}")
            else:
                logger.warning(f"‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω ‚Äî —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ {chat_id}")
        except Exception as e:
            logger.exception(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –≤ {chat_id}: {e}")


# https://cbr.ru/Queries/UniDbQuery/DownloadExcel/125022?Posted=True&From=11.01.2010&To=30.04.2025&I1=true&M1=true&M3=true&M6=true&FromDate=01%2F11%2F2010&ToDate=04%2F30%2F2025

###########################################################################################################################AI

# ---------- –æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä–æ–º ----------

from functions.ai import run_brief


def send_ai(client, recipients):
    import os, json
    from datetime import datetime

    TELEGRAM_LIMIT = 4000

    def normalize_code_fences(text: str) -> str:
        if text.count("```") % 2 == 1:
            text += "\n```"
        return text

    def split_chunks(text: str, limit: int):
        parts, cur = [], ""
        for para in text.split("\n\n"):
            if len(cur) + len(para) < limit:
                cur += para + "\n\n"
            else:
                parts.append(cur.strip())
                cur = para + "\n\n"
        if cur.strip():
            parts.append(cur.strip())
        return parts

    try:
        text, usage, data = run_brief()
    except Exception as e:
        for r in recipients:
            client.send_message(r, f"‚ùå –û—à–∏–±–∫–∞ AI-brief: {e}")
        return

    text = normalize_code_fences(text)
    chunks = split_chunks(text, TELEGRAM_LIMIT)

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_dir = os.path.join("src", "ai")
    os.makedirs(out_dir, exist_ok=True)

    md_path = os.path.join(out_dir, f"ai_brief_{ts}.md")
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(text + "\n\n" + usage)

    json_path = None
    if isinstance(data, dict):
        json_path = os.path.join(out_dir, f"ai_brief_{ts}.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    for r in recipients:
        for i, c in enumerate(chunks, 1):
            prefix = f"–ß–∞—Å—Ç—å {i}/{len(chunks)}\n\n" if len(chunks) > 1 else ""
            # client.send_message(r, prefix + c, parse_mode="markdown", disable_web_page_preview=True)
            from pyrogram.enums import ParseMode
            client.send_message(r, prefix + c, parse_mode=ParseMode.MARKDOWN, disable_web_page_preview=True)

            # client.send_message(r, prefix + c, disable_web_page_preview=True)


        if usage:
            client.send_message(r, usage)

        client.send_document(r, md_path, caption="üìÑ AI-brief (.md)")
        if json_path:
            client.send_document(r, json_path, caption="üßæ AI-brief (.json)")



###########################################################################################################################AI


#################################### –í–µ—Ä–Ω—É—Ç—å ######
# check_if_need_new_rec()

# analitics()  # –õ–∏–±–æ –ø–µ—Ä–µ–¥–µ–ª–∞—Ç—å 
#################################### –í–µ—Ä–Ω—É—Ç—å ######


###### –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è ####
# import subprocess
# import os
# import sys

# def check_git_update(commit_file="log/current_commit.txt"):
#     try:
#         # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–∞–ø–∫–∞ log —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
#         os.makedirs(os.path.dirname(commit_file), exist_ok=True)

#         # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π –∫–æ–º–º–∏—Ç —Å origin
#         subprocess.run(["git", "fetch"], check=True)
#         new_commit = subprocess.check_output(
#             ["git", "rev-parse", "origin/main"], text=True
#         ).strip()

#         # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞—ë–º –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–µ–∫—É—â–∏–π –∫–æ–º–º–∏—Ç
#         if not os.path.exists(commit_file):
#             with open(commit_file, "w") as f:
#                 f.write(new_commit)
#             logger.info(f"üìÑ –§–∞–π–ª {commit_file} —Å–æ–∑–¥–∞–Ω. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∫–æ–º–º–∏—Ç: {new_commit}")
#             return None  # –ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ ‚Äî –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è

#         # –°—á–∏—Ç—ã–≤–∞–µ–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –∫–æ–º–º–∏—Ç
#         with open(commit_file, "r") as f:
#             last_commit = f.read().strip()

#         if new_commit != last_commit:
#             logger.info(f"üîÑ –û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–æ–≤—ã–π –∫–æ–º–º–∏—Ç: {new_commit}")
#             return new_commit
#         else:
#             logger.info("‚úÖ –í–µ—Ä—Å–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–∞. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
#             return None

#     except Exception as e:
#         logger.exception("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è Git:")
#         return None


# def update_and_restart(new_commit, commit_file="log/current_commit.txt"):
#     try:
#         subprocess.run(["git", "pull"], check=True)

#         with open(commit_file, "w") as f:
#             f.write(new_commit)

#         logger.info("‚ôªÔ∏è –ü—Ä–æ–µ–∫—Ç –æ–±–Ω–æ–≤–ª—ë–Ω. –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º...")
#         os.execv(sys.executable, ['python'] + sys.argv)

#     except Exception as e:
#         logger.exception("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–µ:")



# commit_file = "log/current_commit.txt"
# new_commit = check_git_update(commit_file)
# if new_commit:
#     update_and_restart(new_commit, commit_file)



################################################################################–í–ï–†–ù–£–¢–¨
from functions.auto_update import check_and_restart_if_updated
check_and_restart_if_updated()
#################################################################################–í–ï–†–ù–£–¢–¨
# ######

load_dotenv()  

api_hash = os.getenv('api_hash')
for_whom = os.getenv('for_whom')
api_id = os.getenv('api_id')
bot_token = os.getenv('bot_token')

recipients_raw = os.getenv("for_whom_list", "")
recipients = [r.strip() for r in recipients_raw.split(",") if r.strip()]
if not recipients:
    raise ValueError("‚ùå –ù–µ—Ç –ø–æ–ª—É—á–∞—Ç–µ–ª–µ–π. –£–±–µ–¥–∏—Å—å, —á—Ç–æ for_whom_list –∑–∞–¥–∞–Ω –≤ .env")




from pyrogram import Client, idle

#################  –≤–µ—Ä–Ω—É—Ç—å 
client = Client(name='me_client', api_id=api_id, api_hash=api_hash, bot_token = bot_token )
# –ó–∞–ø—É—Å–∫ –∫–ª–∏–µ–Ω—Ç–∞
client.start()

        


check_if_need_new_rec()
send_info_ruonia(client, recipients)

time.sleep(10)
send_ai(client, recipients)

time.sleep(10)
send_wordstat_digest(client, recipients)



time.sleep(10)
send_incidents_daily_regions_digest(client, recipients, phrases=["–ø–æ–∂–∞—Ä", "–≤–∑—Ä—ã–≤", "–±–ø–ª–∞"], top_n_regions=25)


# idle()

# –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏
client.stop()

#################### –≤–µ—Ä–Ω—É—Ç—å 







