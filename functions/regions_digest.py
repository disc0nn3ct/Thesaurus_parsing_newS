# functions/regions_digest.py
# –¢—Ä–µ–±—É–µ–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
#   pip install plotly kaleido geopandas shapely pyproj numpy pyarrow
#
# –§–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø–æ–ª–æ–∂–∏—Ç—å –≤:
#   src/wordstat/geo/
#     - russia_regions.geojson           (–∏–∑ —Å—Ç–∞—Ç—å–∏)
#     - (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) russia_regions.parquet  (–±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)
#
# –í–ê–ñ–ù–û: –µ—Å–ª–∏ —É —Ç–µ–±—è geo-—Ñ–∞–π–ª—ã –ª–µ–∂–∞—Ç –≤ functions/src/wordstat/geo/,
# —Ç–æ –ª–∏–±–æ –ø–µ—Ä–µ–Ω–µ—Å–∏ –∏—Ö –≤ src/wordstat/geo/, –ª–∏–±–æ –∏–∑–º–µ–Ω–∏ WORDSTAT_SAVE_DIR –Ω–∏–∂–µ.

import os, re, json, time, hashlib
from datetime import datetime, timedelta

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import plotly.graph_objects as go

import geopandas as gpd
from shapely.geometry import Polygon, MultiPolygon

# 1) –ø–æ–ø—Ä–∞–≤—å —ç—Ç–æ—Ç –∏–º–ø–æ—Ä—Ç –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª, –≥–¥–µ –ª–µ–∂–∏—Ç _wordstat_post
from functions.wordstat_api import _wordstat_post

try:
    from loguru import logger
except Exception:
    import logging
    logger = logging.getLogger(__name__)

# --- PATHS ---

# –ö–£–î–ê –°–û–•–†–ê–ù–Ø–ï–ú –†–ï–ó–£–õ–¨–¢–ê–¢–´ (–∫–∞—Ä—Ç–∏–Ω–∫–∏, raw –∏ —Ç.–ø.) ‚Äî –ö–ê–ö –†–ê–ù–¨–®–ï
WORDSTAT_SAVE_DIR = os.path.join("src", "wordstat")
WORDSTAT_MAPS_DIR = os.path.join(WORDSTAT_SAVE_DIR, "maps")

# –û–¢–ö–£–î–ê –ë–ï–†–Å–ú –ì–ï–û-–î–ê–ù–ù–´–ï (—Ç–æ–ª—å–∫–æ —á—Ç–µ–Ω–∏–µ)
WORDSTAT_GEO_DIR = os.path.join("functions", "src", "wordstat", "geo")

# —Å–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
os.makedirs(WORDSTAT_MAPS_DIR, exist_ok=True)
os.makedirs(WORDSTAT_GEO_DIR, exist_ok=True)

# --- GEO FILES ---

REGIONS_GEOJSON = os.path.join(WORDSTAT_GEO_DIR, "russia_regions.geojson")
REGIONS_PARQUET = os.path.join(WORDSTAT_GEO_DIR, "russia_regions.parquet")

WORDSTAT_TREE_PATH = os.path.join(WORDSTAT_GEO_DIR, "wordstat_regions_tree.json")
REGIONID_TO_MAPREGION_PATH = os.path.join(WORDSTAT_GEO_DIR, "regionid_to_mapregion.json")
POPULATION_PATH = os.path.join(WORDSTAT_GEO_DIR, "population_by_regionid.json")


# ----------------------------
# helpers
# ----------------------------
def slugify_phrase(s: str, max_len: int = 60) -> str:
    s = (s or "").strip().lower()
    base = re.sub(r"[^\w.-]+", "_", s, flags=re.UNICODE).strip("_")
    if not base:
        base = "phrase"
    if len(base) > max_len:
        base = base[:max_len].rstrip("_")
    h = hashlib.md5(s.encode("utf-8")).hexdigest()[:8]
    return f"{base}_{h}"


def _daily_bounds_last_60_days():
    """
    period=daily: –¥–æ—Å—Ç—É–ø–Ω—ã –¥–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π.
    toDate –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é = –≤—á–µ—Ä–∞, –Ω–æ –∑–∞–¥–∞–¥–∏–º —è–≤–Ω–æ.
    """
    today = datetime.today().date()
    to_date = today - timedelta(days=1)          # –≤—á–µ—Ä–∞
    from_date = to_date - timedelta(days=59)     # 60 –¥–Ω–µ–π –≤–∫–ª—é—á–∞—è to_date
    return from_date, to_date


def _normalize_region_name(s: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è:
    - lower
    - —ë->–µ
    - —É–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏/—Å–∫–æ–±–∫–∏
    - —Å—Ö–ª–æ–ø—ã–≤–∞–µ–º –ø—Ä–æ–±–µ–ª—ã
    """
    s = (s or "").strip().lower()
    s = s.replace("—ë", "–µ")
    s = re.sub(r"[\"'¬´¬ª()]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


# ----------------------------
# Geo: geojson -> (x,y) shapes
# ----------------------------
def geom2shape(g):
    """Polygon/MultiPolygon -> x[], y[] —Å None-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏ (–∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ)."""
    if isinstance(g, MultiPolygon):
        x = np.array([], dtype=object)
        y = np.array([], dtype=object)
        for poly in g.geoms:
            xx, yy = poly.exterior.coords.xy
            x = np.append(x, np.array(xx, dtype=object))
            y = np.append(y, np.array(yy, dtype=object))
            x = np.append(x, None)
            y = np.append(y, None)
        return pd.Series([x[:-1], y[:-1]])
    if isinstance(g, Polygon):
        xx, yy = g.exterior.coords.xy
        return pd.Series([np.array(xx, dtype=object), np.array(yy, dtype=object)])
    return pd.Series([np.array([], dtype=object), np.array([], dtype=object)])


def load_regions_shapes(simplify_tol: int = 500, target_crs: str = "EPSG:32646") -> pd.DataFrame:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: region, population, x, y
    –õ–æ–≥–∏–∫–∞:
      1) –µ—Å–ª–∏ –µ—Å—Ç—å russia_regions.parquet (–ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π) ‚Äî —á–∏—Ç–∞–µ–º –µ–≥–æ
      2) –∏–Ω–∞—á–µ —á–∏—Ç–∞–µ–º russia_regions.geojson, –¥–µ–ª–∞–µ–º to_crs + simplify + geom2shape,
         –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º parquet –∫–∞–∫ –∫—ç—à.
    """
    if os.path.exists(REGIONS_PARQUET):
        shapes = pd.read_parquet(REGIONS_PARQUET)
        # –µ—Å–ª–∏ —Å—Ç–∞—Ä—ã–π parquet –±–µ–∑ —Ü–µ–Ω—Ç—Ä–æ–≤ ‚Äî –ø–µ—Ä–µ—Å–æ–±–µ—Ä—ë–º –∏–∑ geojson
        if ("cx" not in shapes.columns) or ("cy" not in shapes.columns):
            logger.info("REGIONS_PARQUET –±–µ–∑ cx/cy ‚Äî –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞—é –∏–∑ geojson...")
            os.remove(REGIONS_PARQUET)
            return load_regions_shapes(simplify_tol=simplify_tol, target_crs=target_crs)

        need = {"region", "population", "x", "y", "cx", "cy"}
        miss = need - set(shapes.columns)
        if miss:
            raise ValueError(f"–í {REGIONS_PARQUET} –Ω–µ—Ç –∫–æ–ª–æ–Ω–æ–∫: {miss}")
        return shapes[["region", "population", "x", "y", "cx", "cy"]].copy()

    if not os.path.exists(REGIONS_GEOJSON):
        raise FileNotFoundError(
            f"–ù–µ—Ç {REGIONS_GEOJSON}. –ü–æ–ª–æ–∂–∏ —Ñ–∞–π–ª russia_regions.geojson –≤ {WORDSTAT_GEO_DIR}"
        )

    gdf = gpd.read_file(REGIONS_GEOJSON)
    need = {"region", "population", "geometry"}
    miss = need - set(gdf.columns)
    if miss:
        raise ValueError(f"–í {REGIONS_GEOJSON} –Ω–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ {miss}. –ï—Å—Ç—å: {list(gdf.columns)}")

    # geojson –æ–±—ã—á–Ω–æ –≤ EPSG:4326
    if gdf.crs is None:
        gdf = gdf.set_crs("EPSG:4326")

    gdf = gdf.to_crs(target_crs)

    # —Ü–µ–Ω—Ç—Ä—ã –¥–ª—è –ø–æ–¥–ø–∏—Å–µ–π (–±–µ—Ä—ë–º representative_point —á—Ç–æ–±—ã —Ç–æ—á–∫–∞ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –±—ã–ª–∞ –≤–Ω—É—Ç—Ä–∏ –ø–æ–ª–∏–≥–æ–Ω–∞)
    rp = gdf.geometry.representative_point()
    gdf["cx"] = rp.x
    gdf["cy"] = rp.y

    # —É–ø—Ä–æ—â–∞–µ–º –≥–µ–æ–º–µ—Ç—Ä–∏—é —á—Ç–æ–±—ã plotly –Ω–µ –ª–∞–≥–∞–ª
    gdf["geometry"] = gdf["geometry"].simplify(simplify_tol)

    # –¥–µ–ª–∞–µ–º x/y
    gdf[["x", "y"]] = gdf["geometry"].apply(geom2shape)

    shapes = gdf[["region", "population", "x", "y", "cx", "cy"]].copy()


    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π parquet (—É—Å–∫–æ—Ä—è–µ—Ç –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—É—Å–∫–∏)
    try:
        shapes.to_parquet(REGIONS_PARQUET, index=False)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—ë–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–π —Å–ª–æ–π —Ä–µ–≥–∏–æ–Ω–æ–≤: {REGIONS_PARQUET}")
    except Exception as e:
        logger.warning(f"–ù–µ —Å–º–æ–≥ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å {REGIONS_PARQUET}: {e}")

    return shapes


# ----------------------------
# Wordstat regions tree -> mapping regionId -> region_name
# ----------------------------
def fetch_wordstat_regions_tree(force_refresh: bool = False) -> dict:
    """
    /v1/getRegionsTree: –¥–µ—Ä–µ–≤–æ —Ä–µ–≥–∏–æ–Ω–æ–≤ —Å regionId –∏ –∏–º–µ–Ω–∞–º–∏.
    –ö—ç—à: WORDSTAT_TREE_PATH
    """
    if (not force_refresh) and os.path.exists(WORDSTAT_TREE_PATH):
        try:
            with open(WORDSTAT_TREE_PATH, "r", encoding="utf-8") as f:
                cached = json.load(f)
            # –µ—Å–ª–∏ –≤ –∫—ç—à–µ –≤–¥—Ä—É–≥ –ª–µ–∂–∏—Ç –æ—à–∏–±–∫–∞ ‚Äî –∏–≥–Ω–æ—Ä–∏–º –∫—ç—à
            if isinstance(cached, dict) and ("error" in cached or "errors" in cached):
                logger.warning("Wordstat regions tree cache contains error; refetching with force_refresh=True")
            else:
                return cached
        except Exception as e:
            logger.warning(f"–ù–µ —Å–º–æ–≥ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫—ç—à –¥–µ—Ä–µ–≤–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤: {e}. –ü–µ—Ä–µ–∑–∞–ø—Ä–æ—à—É.")

    data = _wordstat_post("/v1/getRegionsTree", {})  # –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

    try:
        logger.info(f"Wordstat getRegionsTree: type={type(data)}, top_keys={list(data.keys())[:20] if isinstance(data, dict) else 'n/a'}")
    except Exception:
        pass

    # –µ—Å–ª–∏ —Å–µ—Ä–≤–∏—Å –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É ‚Äî –ª–æ–≥–∏—Ä—É–µ–º –∏ –≤—Å—ë —Ä–∞–≤–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º (—á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å —á—Ç–æ –ø—Ä–∏—à–ª–æ)
    if isinstance(data, dict) and ("error" in data or "errors" in data):
        logger.error(f"Wordstat getRegionsTree returned error: {data}")

    try:
        with open(WORDSTAT_TREE_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –¥–µ—Ä–µ–≤–æ —Ä–µ–≥–∏–æ–Ω–æ–≤ Wordstat: {WORDSTAT_TREE_PATH}")
    except Exception as e:
        logger.exception(f"–ù–µ —Å–º–æ–≥ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–µ—Ä–µ–≤–æ —Ä–µ–≥–∏–æ–Ω–æ–≤: {e}")

    return data


def flatten_wordstat_regions(tree_json) -> pd.DataFrame:
    """
    –ü–∞—Ä—Å–∏—Ç —Ä–µ–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç Wordstat:
    - regionId  <- value
    - name      <- label
    """
    rows = []

    def walk(node, parent_id=None, level=0):
        if isinstance(node, dict):
            rid = node.get("value")
            name = node.get("label")

            if rid is not None and name:
                try:
                    rid_int = int(rid)
                except Exception:
                    rid_int = None

                if rid_int is not None:
                    rows.append({
                        "regionId": rid_int,
                        "name": str(name),
                        "parentId": int(parent_id) if parent_id is not None else None,
                        "level": int(level),
                    })
                    parent_for_children = rid_int
                    next_level = level + 1
                else:
                    parent_for_children = parent_id
                    next_level = level
            else:
                parent_for_children = parent_id
                next_level = level

            children = node.get("children")
            if isinstance(children, list):
                for c in children:
                    walk(c, parent_for_children, next_level)

        elif isinstance(node, list):
            for item in node:
                walk(item, parent_id, level)

    walk(tree_json)

    df = pd.DataFrame(rows).drop_duplicates(subset=["regionId"]).reset_index(drop=True)

    if df.empty:
        logger.error("flatten_wordstat_regions: –¥–µ—Ä–µ–≤–æ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–æ—Å—å –≤ –ø—É—Å—Ç–æ—Ç—É")
        return pd.DataFrame(columns=["regionId", "name", "parentId", "level", "name_norm"])

    df["name_norm"] = df["name"].map(_normalize_region_name)
    return df

def get_regionid_to_name(force_refresh_tree: bool = False) -> dict[int, str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç mapping regionId -> name (–∫–∞–∫ –≤ Wordstat)
    –ù–∞–ø—Ä–∏–º–µ—Ä: 225 -> "–†–æ—Å—Å–∏—è"
    """
    tree = fetch_wordstat_regions_tree(force_refresh=force_refresh_tree)
    wdf = flatten_wordstat_regions(tree)
    if wdf.empty:
        logger.warning("get_regionid_to_name: –Ω–µ —Å–º–æ–≥ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–µ—Ä–µ–≤–æ, –ø–æ–¥–ø–∏—Å–∏ –±—É–¥—É—Ç —Ç–æ–ª—å–∫–æ regionId")
        return {}
    return {int(r["regionId"]): str(r["name"]) for _, r in wdf.iterrows()}



def build_regionid_to_mapregion(force_refresh_tree: bool = False) -> dict[int, str]:
    """
    –°—Ç—Ä–æ–∏–º mapping regionId -> region (–∫–∞–∫ –≤ geojson/parquet shapes)
    1) –≥—Ä—É–∑–∏–º shapes (parquet –µ—Å–ª–∏ –µ—Å—Ç—å, –∏–Ω–∞—á–µ geojson -> parquet)
    2) –±–µ—Ä—ë–º –¥–µ—Ä–µ–≤–æ Wordstat
    3) –º–∞—á–∏–º –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º –∏–º–µ–Ω–∞–º
    4) —Å–æ—Ö—Ä–∞–Ω—è–µ–º JSON –≤ src/wordstat/geo/regionid_to_mapregion.json
    """
    if os.path.exists(REGIONID_TO_MAPREGION_PATH) and not force_refresh_tree:
        with open(REGIONID_TO_MAPREGION_PATH, "r", encoding="utf-8") as f:
            raw = json.load(f)
        return {int(k): str(v) for k, v in raw.items()}

    shapes = load_regions_shapes()
    shapes["region_norm"] = shapes["region"].map(_normalize_region_name)

    tree = fetch_wordstat_regions_tree(force_refresh=force_refresh_tree)
    wdf = flatten_wordstat_regions(tree)
    if wdf.empty:
        raise RuntimeError("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –¥–µ—Ä–µ–≤–æ —Ä–µ–≥–∏–æ–Ω–æ–≤ Wordstat.")

    merged = wdf.merge(
        shapes[["region", "region_norm"]],
        left_on="name_norm",
        right_on="region_norm",
        how="left"
    )

    mapping: dict[int, str] = {}
    for _, r in merged.dropna(subset=["region"]).iterrows():
        mapping[int(r["regionId"])] = str(r["region"])

    # –µ—Å–ª–∏ –±—É–¥—É—Ç –Ω–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏—è ‚Äî —Å—é–¥–∞ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä—É—á–Ω—ã–µ —Ñ–∏–∫—Å—ã:
    # mapping[123] = "–ú–æ—Å–∫–≤–∞"

    with open(REGIONID_TO_MAPREGION_PATH, "w", encoding="utf-8") as f:
        json.dump({str(k): v for k, v in mapping.items()}, f, ensure_ascii=False, indent=2)

    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—ë–Ω mapping regionId->mapregion: {REGIONID_TO_MAPREGION_PATH} ({len(mapping)} –∑–∞–ø–∏—Å–µ–π)")
    return mapping


# ----------------------------
# Map plot: choropleth per capita
# ----------------------------
def plot_russia_choropleth_per_capita(
    df_daily: pd.DataFrame,
    phrase: str,
    out_png: str,
    window_days: int = 7,
    top_n_regions: int | None = None,
):
    """
    –ö–∞—Ä—Ç–∞ –†–§ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º:
    value = (—Å—É–º–º–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞ window_days) / –Ω–∞—Å–µ–ª–µ–Ω–∏–µ * 100000
    df_daily: date, count, regionId, phrase
    """
    shapes = load_regions_shapes()
    rid2region = build_regionid_to_mapregion(force_refresh_tree=True)


    sdf = df_daily[df_daily["phrase"] == phrase].copy()
    if sdf.empty:
        logger.warning(f"plot_russia_choropleth_per_capita: –ø—É—Å—Ç–æ –¥–ª—è '{phrase}'")
        return None

    sdf["date"] = pd.to_datetime(sdf["date"])
    last_day = sdf["date"].max()
    start = last_day - pd.Timedelta(days=window_days - 1)

    sdf = sdf[(sdf["date"] >= start) & (sdf["date"] <= last_day)]
    agg = sdf.groupby("regionId", as_index=False)["count"].sum().rename(columns={"count": "count_sum"})

    if top_n_regions is not None:
        top_ids = agg.sort_values("count_sum", ascending=False)["regionId"].head(top_n_regions).tolist()
        agg = agg[agg["regionId"].isin(top_ids)]

    agg["region"] = agg["regionId"].map(rid2region)
    agg = agg.dropna(subset=["region"])

    m = shapes.merge(agg, on="region", how="left")
    m["count_sum"] = m["count_sum"].fillna(0)

    m["value"] = m.apply(
        lambda r: (float(r["count_sum"]) / float(r["population"]) * 100000.0) if r["population"] else 0.0,
        axis=1
    )

    vmin, vmax = float(m["value"].min()), float(m["value"].max())
    denom = (vmax - vmin) if (vmax - vmin) > 1e-9 else 1.0

    colors = [
        (0.0,  "rgb(247,251,255)"),
        (0.25, "rgb(200,221,240)"),
        (0.5,  "rgb(115,179,216)"),
        (0.75, "rgb(49,130,189)"),
        (1.0,  "rgb(8,81,156)"),
    ]

    def pick_color(x01: float) -> str:
        xs = [c[0] for c in colors]
        cs = [c[1] for c in colors]
        i = min(range(len(xs)), key=lambda i: abs(xs[i] - x01))
        return cs[i]

    fig = go.Figure()

    for _, r in m.iterrows():
        val = float(r["value"])
        x01 = (val - vmin) / denom
        fill = pick_color(x01)

        hover = (
            f"<b>{r['region']}</b><br>"
            f"–°—É–º–º–∞ {window_days}–¥: {int(round(r['count_sum']))}<br>"
            f"–ù–∞—Å–µ–ª–µ–Ω–∏–µ: {int(r['population']):,}".replace(",", " ") + "<br>"
            f"–ù–∞ 100k: {val:.2f}"
        )

        fig.add_trace(go.Scatter(
            x=r["x"], y=r["y"],
            name=r["region"],
            text=hover,
            hoverinfo="text",
            mode="lines",
            line=dict(color="grey", width=0.6),
            fill="toself",
            fillcolor=fill,
            showlegend=False,
        ))


        # --- –ø–æ–¥–ø–∏—Å–∏ —Ä–µ–≥–∏–æ–Ω–æ–≤ ---
    # —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –∫–∞—à–∏: –ø–æ–¥–ø–∏—Å–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–æ–≤ —Å –Ω–µ–Ω—É–ª–µ–≤—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º (–∏–ª–∏ —Ç–æ–ø-N)
    labels_df = m.copy()

    if top_n_regions is not None:
        labels_df = labels_df.sort_values("value", ascending=False).head(top_n_regions)
    else:
        labels_df = labels_df[labels_df["value"] > 0]

    # –µ—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –º–Ω–æ–≥–æ ‚Äî –æ–≥—Ä–∞–Ω–∏—á–∏–º (–∏–Ω–∞—á–µ –º–µ–ª–∫–∏–π —Ç–µ–∫—Å—Ç –±—É–¥–µ—Ç –º–µ—à–∞—Ç—å)
    if len(labels_df) > 35:
        labels_df = labels_df.sort_values("value", ascending=False).head(35)

    # –∫–æ—Ä–æ—Ç–∫–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è (–º–æ–∂–µ—à—å —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–∞–≤–∏–ª–∞)
    def short_name(name: str) -> str:
        n = str(name)
        n = n.replace("–æ–±–ª–∞—Å—Ç—å", "–æ–±–ª.").replace("–†–µ—Å–ø—É–±–ª–∏–∫–∞", "–†–µ—Å–ø.")
        n = n.replace("–∞–≤—Ç–æ–Ω–æ–º–Ω—ã–π –æ–∫—Ä—É–≥", "–ê–û").replace("–∫—Ä–∞–π", "–∫—Ä.")
        return n

    fig.add_trace(go.Scatter(
        x=labels_df["cx"],
        y=labels_df["cy"],
        text=labels_df["region"].map(short_name),
        mode="text",
        textposition="middle center",
        hoverinfo="skip",
        showlegend=False,
        textfont=dict(size=10),
    ))


    fig.update_xaxes(visible=False)
    fig.update_yaxes(visible=False, scaleanchor="x", scaleratio=1)
    fig.update_layout(
        title=f"Wordstat: '{phrase}' ‚Äî –Ω–∞ 100k –∂–∏—Ç–µ–ª–µ–π (—Å—É–º–º–∞ {window_days} –¥–Ω–µ–π)",
        margin=dict(l=10, r=10, t=40, b=10),
        width=1100,
        height=650,
    )
    import plotly.io as pio
    # –¥–ª—è snap chromium –æ–±—ã—á–Ω–æ —Ç–∞–∫:
    CHROMIUM = "/snap/bin/chromium"
    if os.path.exists(CHROMIUM):
        try:
            # —É —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏–π plotly/kaleido –∏–º—è –ø–æ–ª—è –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è
            pio.kaleido.scope.chromium_executable = CHROMIUM
        except Exception:
            pass
        try:
            pio.kaleido.scope.executable = CHROMIUM
        except Exception:
            pass
        
    print([x for x in dir(pio.kaleido.scope) if "exec" in x.lower() or "chrome" in x.lower()])

    # –ù–£–ñ–ï–ù kaleido: pip install kaleido
    try:
        fig.write_image(out_png, scale=2)
        return out_png
    except Exception as e:
        logger.exception(f"PNG export failed, fallback to HTML: {e}")
        out_html = out_png.replace(".png", ".html")
        fig.write_html(out_html, include_plotlyjs="embed")
        return out_html


# ----------------------------
# Existing plots: total + heatmap
# ----------------------------
def plot_daily_total_compare(df: pd.DataFrame, phrases: list[str], out_path: str):
    """
    –°—É–º–º–∞—Ä–Ω–∞—è –¥–Ω–µ–≤–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Ä–µ–≥–∏–æ–Ω–∞–º (—Å—É–º–º–∞ –ø–æ regionId) –¥–ª—è –∫–∞–∂–¥–æ–π —Ñ—Ä–∞–∑—ã.
    """
    sdf = df[df["phrase"].isin(phrases)].copy()
    if sdf.empty:
        logger.warning("plot_daily_total_compare: –ø—É—Å—Ç–æ–π df")
        return None

    g = (sdf.groupby(["date", "phrase"])["count"].sum()
         .reset_index()
         .sort_values("date"))

    plt.figure(figsize=(12, 5))
    for ph in phrases:
        gg = g[g["phrase"] == ph]
        plt.plot(gg["date"], gg["count"], linewidth=2, label=ph)

    plt.title("Wordstat daily: —Å—É–º–º–∞—Ä–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –ø–æ —Ç–æ–ø-—Ä–µ–≥–∏–æ–Ω–∞–º")
    plt.xlabel("–î–∞—Ç–∞")
    plt.ylabel("–ß–∏—Å–ª–æ –∑–∞–ø—Ä–æ—Å–æ–≤ (sum count)")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()
    logger.info(f"üíæ daily total compare —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {out_path}")
    return out_path


def plot_daily_heatmap(df: pd.DataFrame, phrase: str, out_path: str, top_regions: int = 25):
    """
    Heatmap: —Å—Ç—Ä–æ–∫–∏=regionId, —Å—Ç–æ–ª–±—Ü—ã=–¥–∞—Ç—ã, –∑–Ω–∞—á–µ–Ω–∏–µ=count.
    –ü–æ–¥–ø–∏—Å–∏ –ø–æ –æ—Å–∏ Y: "–ù–∞–∑–≤–∞–Ω–∏–µ (regionId)".
    """
    sdf = df[df["phrase"] == phrase].copy()
    if sdf.empty:
        logger.warning(f"plot_daily_heatmap: –ø—É—Å—Ç–æ –¥–ª—è '{phrase}'")
        return None

    order = (sdf.groupby("regionId")["count"].sum()
             .sort_values(ascending=False)
             .head(top_regions)
             .index.tolist())
    sdf = sdf[sdf["regionId"].isin(order)]

    pivot = (sdf.pivot_table(index="regionId", columns="date", values="count", aggfunc="sum")
               .fillna(0))

    # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –¥–∞—Ç—ã –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö
    pivot.columns = pd.to_datetime(pivot.columns)

    plt.figure(figsize=(14, max(6, 0.28 * len(pivot.index))))
    plt.imshow(pivot.values, aspect="auto")
    plt.title(f"Wordstat daily heatmap (top {top_regions} regions): {phrase}")
    plt.xlabel("–î–∞—Ç–∞")
    plt.ylabel("–†–µ–≥–∏–æ–Ω (regionId)")

    cols = list(pivot.columns)
    step = max(1, len(cols) // 10)
    xticks = list(range(0, len(cols), step))
    plt.xticks(xticks, [cols[i].strftime("%m-%d") for i in xticks], rotation=45)

    # ‚úÖ –ø–æ–¥–ø–∏—Å–∏ —Ä–µ–≥–∏–æ–Ω–æ–≤
    rid2name = get_regionid_to_name(force_refresh_tree=False)

    def label_rid(rid: int) -> str:
        name = rid2name.get(int(rid))
        return f"{name} ({int(rid)})" if name else str(int(rid))

    plt.yticks(range(len(pivot.index)), [label_rid(rid) for rid in pivot.index])

    plt.colorbar(label="count")
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()

    logger.info(f"üíæ heatmap —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {out_path}")
    return out_path


# ----------------------------
# Wordstat API fetchers
# ----------------------------
def fetch_wordstat_regions_distribution(phrase: str, region_type: str = "regions", devices=None) -> pd.DataFrame:
    """
    /v1/regions: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: regionId, count, share, affinityIndex
    region_type: 'cities' | 'regions' | 'all'
    """
    if devices is None:
        devices = ["all"]

    payload = {
        "phrase": phrase,
        "regionType": region_type,
        "devices": devices,
    }
    data = _wordstat_post("/v1/regions", payload)
    rows = data.get("regions", []) or []
    df = pd.DataFrame(rows)
    if df.empty:
        return pd.DataFrame(columns=["regionId", "count", "share", "affinityIndex"])
    df["regionId"] = df["regionId"].astype(int)
    return df.sort_values("count", ascending=False).reset_index(drop=True)


def fetch_wordstat_daily_dynamics(phrase: str, region_id: int, devices=None) -> pd.DataFrame:
    """
    /v1/dynamics daily –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É —Ä–µ–≥–∏–æ–Ω—É (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 60 –¥–Ω–µ–π).
    """
    if devices is None:
        devices = ["all"]

    from_date, to_date = _daily_bounds_last_60_days()

    payload = {
        "phrase": phrase,
        "period": "daily",
        "fromDate": from_date.strftime("%Y-%m-%d"),
        "toDate": to_date.strftime("%Y-%m-%d"),
        "regions": [int(region_id)],
        "devices": devices,
    }
    data = _wordstat_post("/v1/dynamics", payload)
    dyn = data.get("dynamics", []) or []
    df = pd.DataFrame(dyn)
    if df.empty:
        df = pd.DataFrame(columns=["date", "count", "share"])
    else:
        df["date"] = pd.to_datetime(df["date"])
        df["count"] = df["count"].astype(float)
        df["share"] = df["share"].astype(float)
        df = df.sort_values("date").reset_index(drop=True)

    df["regionId"] = int(region_id)
    df["phrase"] = phrase
    return df


def build_daily_region_dataset(
    phrases: list[str],
    top_n_regions: int = 25,
    region_type: str = "regions",
    devices=None,
    sleep_s: float = 0.25,
) -> pd.DataFrame:
    """
    1) /v1/regions -> –±–µ—Ä—ë–º —Ç–æ–ø-N —Ä–µ–≥–∏–æ–Ω–æ–≤ –∑–∞ 30 –¥–Ω–µ–π
    2) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞: /v1/dynamics daily (60 –¥–Ω–µ–π)
    """
    if devices is None:
        devices = ["all"]

    frames = []
    for phrase in phrases:
        dist = fetch_wordstat_regions_distribution(phrase, region_type=region_type, devices=devices)
        if dist.empty:
            logger.warning(f"/v1/regions –ø—É—Å—Ç–æ –¥–ª—è '{phrase}'")
            continue

        top_regions = dist["regionId"].head(top_n_regions).tolist()

        for rid in top_regions:
            try:
                df = fetch_wordstat_daily_dynamics(phrase, rid, devices=devices)
                if not df.empty:
                    frames.append(df)
                time.sleep(sleep_s)
            except Exception as e:
                logger.exception(f"–û—à–∏–±–∫–∞ daily dynamics '{phrase}' region={rid}: {e}")

    if not frames:
        return pd.DataFrame(columns=["date", "count", "share", "regionId", "phrase"])

    return pd.concat(frames, ignore_index=True)


# ----------------------------
# Main sender
# ----------------------------
def send_incidents_daily_regions_digest(tg_client, recipients, phrases=None, top_n_regions: int = 25):
    """
    –î–Ω–µ–≤–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ + –∫–∞—Ä—Ç–∞ –†–§ (–Ω–∞ 100k) + heatmap –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º –¥–ª—è –∏–Ω—Ü–∏–¥–µ–Ω—Ç–Ω—ã—Ö —Å–ª–æ–≤.
    –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: ["–ø–æ–∂–∞—Ä", "–≤–∑—Ä—ã–≤"]
    """
    if phrases is None:
        phrases = ["–ø–æ–∂–∞—Ä", "–≤–∑—Ä—ã–≤"]

    df = build_daily_region_dataset(
        phrases=phrases,
        top_n_regions=top_n_regions,
        region_type="regions",
        devices=["all"],
        sleep_s=0.25,
    )

    if df.empty:
        msg = "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å daily –¥–∏–Ω–∞–º–∏–∫—É –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º (df –ø—É—Å—Ç–æ–π)."
        for chat_id in recipients:
            try:
                tg_client.send_message(chat_id, msg)
            except Exception as e:
                logger.exception(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ {chat_id}: {e}")
        return

    ts = datetime.now().strftime("%Y%m%d_%H%M%S")

    out_total = os.path.join(WORDSTAT_MAPS_DIR, f"daily_total_incidents_{ts}.png")
    total_path = plot_daily_total_compare(df, phrases, out_total)

    map_paths = []
    for ph in phrases:
        out_map = os.path.join(WORDSTAT_MAPS_DIR, f"daily_map_{slugify_phrase(ph)}_{ts}.png")
        mp = plot_russia_choropleth_per_capita(
            df_daily=df,
            phrase=ph,
            out_png=out_map,
            window_days=7,
            top_n_regions=None
        )
        if mp:
            map_paths.append((ph, mp))

    heat_paths = []
    for ph in phrases:
        out_hm = os.path.join(WORDSTAT_MAPS_DIR, f"daily_heatmap_{slugify_phrase(ph)}_{ts}.png")
        hp = plot_daily_heatmap(df, ph, out_hm, top_regions=top_n_regions)
        if hp:
            heat_paths.append((ph, hp))

    from_date, to_date = _daily_bounds_last_60_days()
    header = (
        "üó∫Ô∏è Wordstat: –¥–Ω–µ–≤–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º (—Ç–æ–ø —Ä–µ–≥–∏–æ–Ω–æ–≤)\n"
        f"–§—Ä–∞–∑—ã: {', '.join(phrases)}\n"
        f"–ü–µ—Ä–∏–æ–¥: {from_date.strftime('%Y-%m-%d')} .. {to_date.strftime('%Y-%m-%d')}\n"
        "–ò—Å—Ç–æ—á–Ω–∏–∫: /v1/regions (—Ç–æ–ø –∑–∞ 30 –¥–Ω–µ–π) + /v1/dynamics daily (60 –¥–Ω–µ–π)\n"
        "–§–æ—Ä–º–∞—Ç: —Å—É–º–º–∞—Ä–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ + –∫–∞—Ä—Ç–∞ –†–§ (–Ω–∞ 100k) + heatmap (regionId √ó –¥–∞—Ç–∞)."
    )

    for chat_id in recipients:
        try:
            tg_client.send_message(chat_id, header)

            if total_path:
                tg_client.send_photo(chat_id, photo=total_path)

            for ph, path in map_paths:
                tg_client.send_photo(chat_id, photo=path, caption=f"–ö–∞—Ä—Ç–∞ –†–§: {ph}")

            for ph, path in heat_paths:
                tg_client.send_photo(chat_id, photo=path, caption=f"Heatmap: {ph}")

            logger.info(f"‚úÖ incidents daily+regions –¥–∞–π–¥–∂–µ—Å—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ {chat_id}")
        except Exception as e:
            logger.exception(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ incidents –¥–∞–π–¥–∂–µ—Å—Ç–∞ –≤ {chat_id}: {e}")
